%PUT STG, COG, ADG as option
\documentclass[COG,11pt]{ercgrant}
% put here the year of the call
\renewcommand{\callyear}{2023}
\setmainfont{Arial}
\bibliography{bibliography.bib}

\author{Fabian Sinz}
\acro{Circuits 2 Behavior}
\title{Linking neural variability to perception, anatomy, and behavior by data-driven models of the mouse visual system}
\institution{Georg August Universität Göttingen}

% ====== BODY OF THE DOCUMENT ======
\begin{document}

\maketitle

\begin{abstract}
	\textcolor{red}{
		\input{summary.tex}
	}
\end{abstract}


%%%%%%%%%%%%% EXTENDED SYNOPSIS %%%%%%%%%%%%%%%%%%%

\section{Extended Synopsis of the Scientific Proposal}

% neural variability under natural conditions
% - determined by "unconscious inference" (i.e. vision)
% - depends on the features neurons present (i.e. area)
% - determined by a hidden task
% - reflected in the neuro-anatomical circuitry
% - governed by principles of Bayesian inference (possibly with the wrong model)
% perceptlets
\subsection{Background, state of the art and rationale}
\textit{The whole is more than the sum of its parts} -- this phrase from Aristotle's Metaphysics Book VIII is an apt description of visual perception and behavior. 
Each single neuron in the visual system is only sensitive to a mosaic piece of the visual field. Only the concerted interactions of all these sensory fragments together creates our perception, forming hypotheses about the world beyond our eyes which builds the informational basis of our behavior. 
However, our perception is not a static image processing algorithm: Behavioral goals, attentional states and our current expectation about the environment can influence how visual information is acquired and processed, changing the activity profile of hundred thousands of neurons it relies on with it. 
Even if the visual stimulus is identical the neural activity in the visual system and our perception of the world can change. This allows the brain to flexibly interpret sensory information and adapt to the particular behavioral context \hl{Include image of bistable percept?}.  
\textbf{The central hypothesis for the research I propose here is that  neuronal variability reflects fundamental computational processes to infer properties of the natural environment beyond the eyes. My goal is to link the structure of this neural variability to mathematical principles of inference under uncertainty, neuro-anatomical microcircuit connectivity, and behavioral goals of the organism.} 

% Current state: mostly images, variability only for simple stimuli or ignored, behavioral mostly ignored
% Visual system, neural variability, studied in isolation
The goal of the visual and other sensory systems is to provide the organism with actionable information about the world inferred from sensory data \hl{ref}. \hl{Include active sensing?} 
The visual cortex roughly contains between 50,000 and 200,000 neurons under each mm$^2$ of cortex surface~\parencite{Colonnier1981-oj}.
Only a small fraction of these neurons are functionally characterized, because -- until recently -- the number of neurons that could be recorded was limited. 
Most of what we currently know about the visual system is based on simple stimuli, such as dots or gratings, or static natural images. 
However, the most ethological stimulus for the visual system -- natural video -- remains largely unexplored because of the complexity of the stimulus and the resulting difficulty to parametrize it with few interpretable dimensions. 
How the neuronal representation in visual cortex supports and adapts to behavioral goals under natural conditions is largely unknown. 
Furthermore, the variability of neuronal activity around the mean response to a stimulus exhibits rich structure \hl{Stringer} and changes depending on the task context \hl{Haefner, Churchland}.
While the structure of this variability cannot be explained by the stimulus, it can provide the organism with valuable information about the certainty with which particular aspects of the world can be inferred \hl{Ma PPC, Pouget}\hl{Prior expectations about the stimulus?}
But again, despite a few exceptions \hl{Gallant, Bashiri} the characterization and the computational role of stimulus conditioned variability for natural images and video remains unexplored. 

Studying the role of neuronal variability for natural stimuli and behavior in controlled experimental conditions is next to impossible because the same stimulation and behavior cannot be repeated multiple times. 
At the same time, the required experimental methods for simultaneously observing behavior, large scale neuronal activity, and its circuit implementation under natural conditions do currently not exist. Furthermore, the traditional way of studying sensory systems via carefully designed stimuli and experimental protocols might be too restricted for studying ``natural neuroscience'' since neuronal selectivity can be complex and might not easily map to human interpretable categories~\parencite{Doerig2022-ex}. 

However, recent success in modeling neuronal activity and behavior with deep neuronal networks offer an promising avenue to overcome the above challenges. We and others have demonstrated that it is possible to learn general models from large scale neuronal data that can predict the responses of thousands of neurons to arbitrary natural stimuli and can be used to derive novel and experimentally testable predictions \textit{in vivo} from studying the model \textit{in silico}.
Since these models -- in the ideal case -- copy the functional behavior of large scale neuronal populations, we refer to them as \emph{functional digital twins}. The goal of these models is not to simulate neuronal activity in a biophysical simulation of neuronal circuits, but rather to use machine learning as a tool to fill in the gaps of our ignorance about the biological system between measurable quantities such as visual input, neuronal activity, and behavior, and tie  multiple experiments together in one model that can be use to derive novel and readily testable hypothesis about the computations underlying perception and behavior. 

This approach relies on \circled{1} the availability of massive amounts of measurements from the biological system and \circled{2} tools from modern machine learning and neural data science. 
With regards to \circled{1}, I was involved in conceptually shaping and running a large consortium\footnote{\url{https://www.ninai.org/\#team}} that acquired the activity and detailed circuit neuroanatomy of about 60,000 neurons in the visual cortex of a mouse. The activity was recorded to natural videos, rendered video of naturalistic scenes, and classical stimuli. \hl{unprecedented to before}. Thus, this data allows us to connect latent properties of the world (object ID, geometrical properties from rendering), natural vision and neuronal anatomy. In auxiliary experiments, the consortium recorded over \hl{XXX} hours of neuronal activity in over \hl{XXX} unique neuronal from mouse visual cortex. \hl{behavioral data}. \hl{I have access to all this data}. 

Complex datasets like these put neuroscience in the area of big data and cannot be simply analyzed by techniques that focus on single neurons and low parametric stimuli, but require tools of modern machine learning and neural data science to connect and interpret the different high dimensional observations. My work over the last five years has focussed on exactly that. We built deep predictive models for mouse visual cortex to natural images and video that yielded novel and experimentally verified insights \parencite{Walker2019-yw, Franke2022-do, Sinz2018-sk}. \hl{more details}

My previous work has demonstrated that functional digital twins of the visual system can be learned from data and provide novel insights. Their key advantage is that they can compile multiple experiments into a single model. This model is flexible enough to extract patterns from the data in a more unbiased way and does not rely on manual design of low parametric models. Instead, these models let the data speak for itself and extract complex but verifiable hypotheses about the sensory system. Here I propose to take several logical next steps in the development of digital twins and use them to study the role neuronal variability in perception and behavior, and its foundation in the neuro-anatomical circuit structure. 


\subsection{Approach}
My objectives is to build data-driven functional digital twins for mouse visual cortex and behavior, and use them to understand the role of neuronal variability in perceptual inference and behavior, as well as its relation to neuronal circuit anatomy. The scientific questions all require substantial methodological innovations in machine learning for neuroscience. Thus each objective has a technical goal and a scientific question. 

\subsubsection{Objectives}
{\def\arraystretch{1.5}\tabcolsep=5pt
\begin{tabularx}{\textwidth}{l|X|X}
 & \textbf{technical goal} & \textbf{scientific question} \\\hline
\obji 
& Build a dynamic model of neuronal firing and its variability for the entire visual cortex of the mouse. 
& Is stimulus conditioned neural variability stimulus dependent? \hl{brain areas?}\\\hline
\objii & 
Build a joint generative model of videos and neuronal responses.&
Are neural fluctuations consistent with probabilistic inference? \\\hline
\objiii & 
Build a graph neural network based predictor of correlation matrices. &
What part of the stimulus-conditioned correlations can be explained by neuro-anatomical microcircuit structure? \\\hline
\objiv & 
Build a digital twin of mouse with neuronal activity and behavior. &
Are perceptual uncertainties on a neural level consistent with behavioral uncertainties? \\
\end{tabularx}
}

\obji~builds the backbone for \objii,~\objiii,~and~\objiv. \objii~relates the neuronal activity and its variability compiled into the digital twin to mathematical principles of perceptual inference under uncertainty. \objiii~investigates the anatomical basis of the statistical structure in the neuronal variability in the local cortical microcircuit. \objiv~uses joint behavioral and functional recordings to relate neuronal variability to perceptual goals of a behaving mouse. 

\subsubsection{Interdisciplinarity}
This project addresses fundamental research questions in neuroscience by developing novel machine learning models. The successful execution of this project requires strong expertise in both fields and contributes to advancing both of them. I am trained in computer science/bioinformatics (undergraduate), machine learning (since undergraduate), computational neuroscience (PhD), and neuroscience (PostDocs). I have a track record of over \hl{X} peer reviewed papers on machine learning and neuroscience published in high ranking journals and top tier conferences together with my long term collaborators (Drs. Tolias and Franke, Baylor College of Medicine, Houston, TX, and Emanoulli Froudarakis, FORTH, Crete). I will build on this foundation to successfully implement the proposed goals of this project. 

\subsubsection{Beyond state of the art}
\hl{data and data science}
Here I propose to build on our foundational work for functional digital twins and substantially extend them in several directions: video input, single trial variability, circuit connectivity, and behavior. As the volume and detail of data in neuroscience is increasing fast, studying neural systems under natural conditions will require models that can integrate these multiple measurements. Because of the complexity of both the neural system and the stimulus domain, we still know for to little about the system to manually craft a model. Thus the only way to obtain a model with substantial predictive power is to learn it from data -- create a functional digital twin. The current proposal takes several logical next steps to extend this approach and provide new insights about the neuronal computations that underlie our perception and behavior. 
\hl{make more concise why this is \textbf{beyond} SOTA. }

\subsubsection{Potential impact}

\subsubsection{\colorbox{obji}{\color{white}Objective 1}: A video-driven latent state model of mouse visual cortex}
\label{sub:obji}

\underline{Overview and rationale:}
Deep learning has set new standards in encoding models that can predict neural responses of thousands of neurons from arbitrary pixel-based visual input. However, almost all encoding models for visual cortex are based on natural images only and cannot account for the most ethological input to the visual system: video. At the same time, most models focus on predicting a single activity vector for a given input (the mean activity), ignoring trial to trial fluctuations\hl{neural variability for the same input}. I will build on video based encoding models~\parencite{Sinz2018-sk}\todo{ref and figure from Eric} and on image based encoding models for neural variability~\parencite{Bashiri2021-or} my team and I built in recent years, to develop a \emph{video-driven latent state encoding model} for hundreds of thousands of neurons from the entire mouse visual cortex that can (i) predict responses of thousands of neurons to arbitrary videos, (ii) predicts a joint distribution for the activity of all neurons for every given time point, and (iii) contains a multi-dimensional latent state to model unobserved internal brain states. The model will be jointly trained on multiple scans from visual cortices of several mice watching natural video. This model will allow us to explore the statistical dependencies in the trial to trial variability \textit{in silico} and relate it to normative first principles (\objii) and microcircuit connectivity (\objiii), and serve as the feature extractor for the mouse imitation model (\objiv). 

\underline{Scientific goal:} Develop a data-driven video-based model of excitatory neurons in mouse visual cortex that predicts the full distribution of neural variability at any given time for any given video.


\underline{Approach:} The proposed models will consist of two parts:

A \textit{common feature extractor} that extracts nonlinear features for each brain area from a given video using 3D convolutional and recurrent deep networks\todo{include figure}. Because all neurons of one area are predicted from the same feature representation, it can learn a characterisitc set of features from neural recordings~\parencite{Lurz2020-ua}. We will explore two versions of this feature extractor: one strictly hierarchical where consecutive where each layer only receives input from lower layers, and one with feedback where a lower layer can receive inputs from past features of higher layers -- modeling the recurrent feedback structure in the brain. The feature extractor will be trained on responses of thousands of neurons to natural and parameteric videos. Since the feature extractor is shared, it can also be shared across neurons from different experiments with possibly different stimuli. 
    
A \textit{readout mechanism} that predicts a the joint response distribution of the modeled neural population across neurons and frames. Here we will extend our previous model for response distribution on natural image~\parencite{Bashiri2021-or} and us a normalizing flow model~\parencite{Rezende2015-mx} to transform neural responses into a correlated latent Gaussian distribution. Developing this mechanism poses four challenges: \circled{1} we want to be able to compute conditional distribution (e.g. conditioned on past time or other neurons), \circled{2} it needs to be efficient to run on videos, \circled{3} the correlations should be possibly stimulus dependent, and \circled{4} it needs to deal with the (semi)-discrete nature of deconvolved Calcium recordings (peak at zero, but otherwise continuous density). We will deal with \circled{1} by learning a dimensionwise normalizing flow such as in \cite{Bashiri2021-or}, \circled{2} and \circled{3} by using a Gauss Markov model (Kalman Filter) that also receives inputs from the video feature extractor, and \circled{4} by using variational dequantization~\parencite{Hoogeboom2021-zs} to turn the discrete part into a continuous signal.  \hl{include scientific part + areas}

\underline{Technical innovation:}


% feedback, no feedback
\underline{Expected outcome:} A ``functional digital twin'' of the visual system of the mouse that \circled{1} predicts the response of real neurons in all cortical areas of the mouse visual system to arbitrary videos, \circled{2} model correlated stimulus-independent fluctuations caused by unobserved brain states, and \circled{3} can correctly predict stimulus-conditioned fluctuations around the mean response of the population to a video. 
\hl{neurons from different areas have not negative noise correlations}

\subsubsection{\colorbox{objii}{\color{white}Objective 2}: Normative theories for neural variability}
\label{sub:objii}
\underline{Overview and rationale:} At the heart of all normative theories for perception is the idea that sensory systems infer properties about the world from the data provided to us by our senses. For instance, we are interested in the identity, properties, and location of objects that we infer from noisy activities of the photoreceptors in our eyes. Because the mapping from world to senses looses information (e.g. there are many possible objects consistent with the image on the retina), inferring properties world inherently needs to deal with uncertainty and can be formalized in the framework of Bayesian inference. Almost all theories that formalize perceptual inference on a neuronal level, make predictions about stimulus conditioned the neural variability. However, models in those theories are usually simplified and only qualitatively compared to neuronal recordings. In this objective, I propose to \textit{test predictions of the normative theory of posterior inference} using the model from \obji~and neuronal responses to rendered stimuli, for which the properties of the world behind the image are known. 

\underline{Scientific goal:} Characterize the uncertainty about latent world properties expressed by neuronal variability using the model from \obji~ and neuronal response to rendered stimuli. Test predictions made by the normative theory of posterior inference~\hl{ref} with real recorded data. 


\underline{Approach:}
We will extend the model of \obji~by another module that decodes a video of latent world properties from the latent representation of the model. To train this additional part of the model, we will use rendered videos from the MICrONs dataset and extract pixelwise properties from them such as depth, slant, curvature, object identity, or optical flow. Importantly, the rest of the model will stay fixed so that the latent representation represents neuronal features and cannot adapt to reconstructing world properties. Given all these components we can then compute a distribution of the latent feature of the model, given the video and the neural responses, and can propagate uncertainty about them into uncertainty about world features given the neuronal responses and the input video. From that we have a direct prediction how the uncertainty in the neuronal response translates into uncertainty on world features. In addition, we can also change the image to manipulate uncertainties about selected world features. This produces image that can then later be tested in behavioral experiments. 

Using this model, we will test two predictions that can be derived from the idea that the sensory system infers a representation of the posterior over world properties given the visual input. \circled{1} Some world properties, such as object identity or local curvature, are constant in time. A posterior representation of these properties can only become more certain over time, as more information is acquired. We will test this by looking at the uncertainty of selected objects over the course of a video. \circled{2} If two neurons stand for the same world feature, then several versions of the theory predict that the neuronal responses should be \textit{anti-correlated given that stimulus} because of the probabilistic effect of ``explaining away''. We will test whether these effects can be found in the model. In addition, we can optimize stimuli so that two neurons become maximally anti-correlated. 

\underline{Technical innovation:} A novel models that combines rendered videos and their latent geometric and semantic information with a dynamic latent state model of neural variability into a model of perceptual uncertainty.

\underline{Expected outcome:} We expect to find effects of decreasing uncertainty and explaining away in the developed model of perceptual uncertainty. \hl{cite Christoph}

\subsubsection{\colorbox{objiii}{\color{white}Objective 3}: Explain neural variability from microcircuit connectivity}
\label{sub:objiii}
\underline{Overview and rationale:} 
\obji~and~\objii~investigate the functional aspects of neuronal variability and their relation to perception. At the same time, the statistical structure of neuronal variability must derive from the structure of the local microcircuit. If two neurons are connected or receive input from the same neuron, then they will likely inhibit correlated fluctuations even for a fixed sensory input, because random noise events in one neuron can lead to activity in the downstream neurons. Or -- to put it in technical terms -- if we had the causal graph of a network, we could predict resulting correlations in neuronal variability. However, in real neurons \textit{in vivo} it is unknown how much of the correlations in neural variability can be explained by the local circuit connectivity. Here I propose to quantify how much of the correlations in variability can be explained by the local microscircuit graph structure using graph neural networks and the connectivity from the MICrONS dataset. This will allow us to determine what components in the local circuit are most influential on correlated neural variability. This will link normative theories in \objii~to the neuroanatomy of the microcircuit in a real visual system. 

\underline{Scientific goal:} Find a link between functional relation and anatomical connectivity in real circuits of neurons. 

\underline{Approach:} We will extract the local circuit connectivity from the reconstructed neurons in the volume of the platinum mouse. Each neuron will be labeled according to its identified type (pyramidal neuron, basket cell interneuron, ...). Note that we will have responses to natural stimuli for the excitatory neurons, but not the inhibitory neurons. Thus, the circuit graph will contain more neurons than the correlation matrix for neuronal fluctuations. Also note that not all neurons in the graph will be connected. This means we will obtain several disconnected local microcircuits that we can use for training and testing the networks proposed below. 

We will explore two approaches to test the explanatory power of local connectivity for the statistical structure in neuronal variability: \circled{1} We will extend the readout of the network of~\obji~by another component the predicts the stimulus conditioned correlations (specifically: a sparse precision matrix) from the connectivity, the feature selectivity of the neuron, and the current visual input using a graph neuronal network~\parencite[GNN]{Scarselli2009-gy}. We will model the feature selectivity via the feature weight for the mean stimulus driven response in the network of \obji. Note that this approach allows us to predict the precision matrices for new, previously unobserved, neurons and circuits. Thus, we will train the model on a subset of the microcircuits and test it on a held-out set of circuits. In approach \circled{2} we will model the response distribution directly based on the graph using our recently developed conditional graph neuronal network flow model~\parencite{Pierzchlewicz2022-tq}. We will quantitatively evaluate both models based on likelihood~\parencite{Lurz2022-up} how well they can predict the fluctuation structure on held out circuits and images. 

We will then subsequently investigate how much of the noise correlations can be explained from the local circuit, and how lesions of the local circuit would affect perceptual inference by combining the model of \objiii~with the approach to predict latent world properties in \objii.

\underline{Technical innovation:} Graph neuronal network model relating neuronal micro circuit anatomy to functional properties and perceptual uncertainties.  

\underline{Expected outcome:} We expect to find canonical circuit patterns that lead to defined noise correlations. For instance, if two excitatpory neurons are reciprocally connected via an inhibitory interneuron (such as a PV neuron/basket cell) then they should exhibit negative noise correlations. Based on our preliminary data \hl{todo microns} we expect this to be dependent on the feature selectivity of the participating neurons and match with the normative theories of posterior coding from~\objii.

\subsubsection{\colorbox{objiv}{Objective 4}: Towards an embodied digital twin of the mouse visual system.}
\label{sub:objiv}
\underline{Overview and rationale:} 
Neuronal activity in visual areas display an intricate interplay with behavior. First of all, motor behavior such as eye movements, determine what stimulus to acquire next which substantially alter the sensory statistics the system is faced with. Secondly, even for a fixed stimulus, behavioral goals can influence neuronal activity from trial to trial \hl{Haefner, Chruchland}. 

\underline{Scientific goal:} 

\underline{Approach:}

\underline{Technical innovation:} 

\underline{Expected outcome:} 

\subsection{Risk Management}


%%%%%%%%%%%%% BIBLIOGRAPHY %%%%%%%%%%%%%%%%%%%
\begin{small}
\printbibliography
\end{small}

% \renewcommand\bibsection{\subsection{\refname}}
% \begin{small}
% 	\bibliographystyle{aa}
% 	\bibliography{bibliography}
% \end{small}

%%%%%%%%%%%%% CURRICULUM VITAE %%%%%%%%%%%%%%%%%%%
\newpage
\section{Curriculum vitae}

\subsection{Personal Information}
\begin{tabular}{p{5cm}l}
	Last name, first name: & \textcolor{red}{Doe, John} \\
	ORCID:                 & \textcolor{red}{fill}      \\
	Address:               & \textcolor{red}{Institute} \\
	                       & \textcolor{red}{Street}    \\
	                       & \textcolor{red}{Place}     \\
	Date of birth:         & \textcolor{red}{fill}      \\
	Nationality:           & \textcolor{red}{fill}      \\
	Website:               & \textcolor{red}{fill}      \\
\end{tabular}

\subsection{Education}
\color{red}
\begin{tabular}{p{2cm}p{12cm}}
	20..
	 & \textbf{\textcolor{red}{Ph.D. Astronomy, A University}} \\
	 & Thesis: \textit{\textcolor{red}{fill}}                  \\
	 & Advisor: \textit{\textcolor{red}{fill}}
\end{tabular}
\color{black}

\subsection{Current Position}
\color{red}
\begin{tabular}{p{2cm}p{12cm}}
	since 20..
	 & \textbf{Barista} at Institute \\
	 & City, Some Country
\end{tabular}
\color{black}

\subsection{Previous Positions}
\color{red}
\begin{tabular}{p{2cm}p{12cm}}
	20.. -- 20..
	             & \textbf{Research Associate} at Institute            \\
	             & City, Somecountry                                   \\
	20.. -- 20.. & \textbf{Postdoctoral Researcher} at other Institute \\
	             & Town, Other Country                                 \\
\end{tabular}
\color{black}

\subsection{Fellowships and Awards}
\color{red}
\begin{itemize}
	\item A
	\item B
\end{itemize}
\color{black}

\subsection{Supervision Of Graduate Students And Postdoctoral Fellows}
\subsection{Teaching Activities}
\subsection{Organization of Scientific Meetings}
\subsection{Institutional Responsibilities}
\subsection{Reviewing Activities}
\subsection{Memberships of Scientific Societies}
\subsection{Major Collaborations}
\subsection{Career Breaks}
\subsection{Covid-19 Impact to Scientific Productivity}

%%%%%%%%%%%%% APPENDIX %%%%%%%%%%%%%%%%%%%
\newpage
\section*{Appendix:\\ All ongoing and submitted grants and funding of the PI (Funding ID)}
\subsection{On-going Grants}
\color{red}
\begin{footnotesize}
	\def\arraystretch{1.5}
	\begin{tabular}{|p{3.9cm}|p{2.5cm}|p{1.4cm}|p{2.cm}|p{1.6cm}|p{1.8cm}|}
		\hline
		\rowcolor{black!20}
		\textbf{Project Title}         &
		\textbf{Funding source}        &
		\textbf{Amount\newline(Euros)} &
		\textbf{Period}                &
		\textbf{Role of the PI}        &
		\textbf{Relation to \newline current ERC \newline proposal}          \\
		\hline
		fill                           & fill & \EUR{1} & fill & fill & fill \\
		\hline
	\end{tabular}
\end{footnotesize}
\color{black}

\subsection{Applications}
\color{red}
\begin{footnotesize}
	\def\arraystretch{1.5}
	\begin{tabular}{|p{3.9cm}|p{2.5cm}|p{1.4cm}|p{2.cm}|p{1.6cm}|p{1.8cm}|}
		\hline
		\rowcolor{black!20}
		\textbf{Project Title}         &
		\textbf{Funding source}        &
		\textbf{Amount\newline(Euros)} &
		\textbf{Period}                &
		\textbf{Role of the PI}        &
		\textbf{Relation to \newline current ERC \newline proposal}          \\
		\hline
		fill                           & fill & \EUR{1} & fill & fill & fill \\
		\hline
	\end{tabular}
\end{footnotesize}
\color{black}

%%%%%%%%%%%%% APPENDIX %%%%%%%%%%%%%%%%%%%
\newpage
\section{Early achievements track-record}

\subsection{Refereed Publications in Major Journals}
\color{red}
Items starting with a solid symbol ($\bullet$, \fstar) are publications without the PhD advisor as
co-author, others ($\circ$, \ostar) include the PhD advisor as co-author. Highlighted papers are
marked with a star symbol (\ostar, \fstar).\\
\color{black}

\color{red}
\begin{itemize}[topsep=0pt,itemsep=0.62ex,partopsep=0ex,parsep=0.5ex]
	\item INCLUDE YOUR PUBLICATION LIST HERE
	      % \input{pub_list.tex}
\end{itemize}
\color{black}

\subsection{Talks}
\subsubsection{\textit{Selected Colloquia}}

\color{red}
\begin{longtable}{p{7.5cm}p{6cm}l}
	Seminar & A University & date \\
	Seminar & B University & date \\
\end{longtable}
\color{black}

\subsection{\textit{Selected Conference Talks}}

\color{red}
\begin{longtable}{p{7.5cm}p{6cm}l}
	Invited review on Blabla & City & date \\
\end{longtable}
\color{black}

\end{document}
