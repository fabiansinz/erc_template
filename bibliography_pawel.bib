
@ARTICLE{Baker2022-ph,
  title         = "Video {PreTraining} ({VPT)}: Learning to Act by Watching
                   Unlabeled Online Videos",
  author        = "Baker, Bowen and Akkaya, Ilge and Zhokhov, Peter and
                   Huizinga, Joost and Tang, Jie and Ecoffet, Adrien and
                   Houghton, Brandon and Sampedro, Raul and Clune, Jeff",
  abstract      = "Pretraining on noisy, internet-scale datasets has been
                   heavily studied as a technique for training models with
                   broad, general capabilities for text, images, and other
                   modalities. However, for many sequential decision domains
                   such as robotics, video games, and computer use, publicly
                   available data does not contain the labels required to train
                   behavioral priors in the same way. We extend the
                   internet-scale pretraining paradigm to sequential decision
                   domains through semi-supervised imitation learning wherein
                   agents learn to act by watching online unlabeled videos.
                   Specifically, we show that with a small amount of labeled
                   data we can train an inverse dynamics model accurate enough
                   to label a huge unlabeled source of online data -- here,
                   online videos of people playing Minecraft -- from which we
                   can then train a general behavioral prior. Despite using the
                   native human interface (mouse and keyboard at 20Hz), we show
                   that this behavioral prior has nontrivial zero-shot
                   capabilities and that it can be fine-tuned, with both
                   imitation learning and reinforcement learning, to
                   hard-exploration tasks that are impossible to learn from
                   scratch via reinforcement learning. For many tasks our
                   models exhibit human-level performance, and we are the first
                   to report computer agents that can craft diamond tools,
                   which can take proficient humans upwards of 20 minutes
                   (24,000 environment actions) of gameplay to accomplish.",
  month         =  jun,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2206.11795"
}



@ARTICLE{Cobbe2020-uk,
  title         = "Phasic Policy Gradient",
  author        = "Cobbe, Karl and Hilton, Jacob and Klimov, Oleg and Schulman,
                   John",
  abstract      = "We introduce Phasic Policy Gradient (PPG), a reinforcement
                   learning framework which modifies traditional on-policy
                   actor-critic methods by separating policy and value function
                   training into distinct phases. In prior methods, one must
                   choose between using a shared network or separate networks
                   to represent the policy and value function. Using separate
                   networks avoids interference between objectives, while using
                   a shared network allows useful features to be shared. PPG is
                   able to achieve the best of both worlds by splitting
                   optimization into two phases, one that advances training and
                   one that distills features. PPG also enables the value
                   function to be more aggressively optimized with a higher
                   level of sample reuse. Compared to PPO, we find that PPG
                   significantly improves sample efficiency on the challenging
                   Procgen Benchmark.",
  month         =  sep,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2009.04416"
}


@ARTICLE{Miyazaki2018-gy,
  title    = "Reward probability and timing uncertainty alter the effect of
              dorsal raphe serotonin neurons on patience",
  author   = "Miyazaki, Katsuhiko and Miyazaki, Kayoko W and Yamanaka, Akihiro
              and Tokuda, Tomoki and Tanaka, Kenji F and Doya, Kenji",
  abstract = "Recent experiments have shown that optogenetic activation of
              serotonin neurons in the dorsal raphe nucleus (DRN) in mice
              enhances patience in waiting for future rewards. Here, we show
              that serotonin effect in promoting waiting is maximized by both
              high probability and high timing uncertainty of reward.
              Optogenetic activation of serotonergic neurons prolongs waiting
              time in no-reward trials in a task with 75\% food reward
              probability, but not with 50 or 25\% reward probabilities.
              Serotonin effect in promoting waiting increases when the timing
              of reward presentation becomes unpredictable. To coherently
              explain the experimental data, we propose a Bayesian decision
              model of waiting that assumes that serotonin neuron activation
              increases the prior probability or subjective confidence of
              reward delivery. The present data and modeling point to the
              possibility of a generalized role of serotonin in resolving
              trade-offs, not only between immediate and delayed rewards, but
              also between sensory evidence and subjective confidence.",
  journal  = "Nat. Commun.",
  volume   =  9,
  number   =  1,
  pages    = "2048",
  month    =  jun,
  year     =  2018,
  language = "en"
}


@ARTICLE{Morris2006-ub,
  title    = "Midbrain dopamine neurons encode decisions for future action",
  author   = "Morris, Genela and Nevet, Alon and Arkadir, David and Vaadia,
              Eilon and Bergman, Hagai",
  abstract = "Current models of the basal ganglia and dopamine neurons
              emphasize their role in reinforcement learning. However, the role
              of dopamine neurons in decision making is still unclear. We
              recorded from dopamine neurons in monkeys engaged in two types of
              trial: reference trials in an instructed-choice task and decision
              trials in a two-armed bandit decision task. We show that the
              activity of dopamine neurons in the decision setting is modulated
              according to the value of the upcoming action. Moreover, analysis
              of the probability matching strategy in the decision trials
              revealed that the dopamine population activity and not the reward
              during reference trials determines choice behavior. Because
              dopamine neurons do not have spatial or motor properties, we
              conclude that immediate decisions are likely to be generated
              elsewhere and conveyed to the dopamine neurons, which play a role
              in shaping long-term decision policy through dynamic modulation
              of the efficacy of basal ganglia synapses.",
  journal  = "Nat. Neurosci.",
  volume   =  9,
  number   =  8,
  pages    = "1057--1063",
  month    =  aug,
  year     =  2006,
  language = "en"
}


@ARTICLE{Botvinick2009-nn,
  title    = "Hierarchically organized behavior and its neural foundations: a
              reinforcement learning perspective",
  author   = "Botvinick, Matthew M and Niv, Yael and Barto, Andew G",
  abstract = "Research on human and animal behavior has long emphasized its
              hierarchical structure-the divisibility of ongoing behavior into
              discrete tasks, which are comprised of subtask sequences, which
              in turn are built of simple actions. The hierarchical structure
              of behavior has also been of enduring interest within
              neuroscience, where it has been widely considered to reflect
              prefrontal cortical functions. In this paper, we reexamine
              behavioral hierarchy and its neural substrates from the point of
              view of recent developments in computational reinforcement
              learning. Specifically, we consider a set of approaches known
              collectively as hierarchical reinforcement learning, which extend
              the reinforcement learning paradigm by allowing the learning
              agent to aggregate actions into reusable subroutines or skills. A
              close look at the components of hierarchical reinforcement
              learning suggests how they might map onto neural structures, in
              particular regions within the dorsolateral and orbital prefrontal
              cortex. It also suggests specific ways in which hierarchical
              reinforcement learning might provide a complement to existing
              psychological models of hierarchically structured behavior. A
              particularly important question that hierarchical reinforcement
              learning brings to the fore is that of how learning identifies
              new action routines that are likely to provide useful building
              blocks in solving a wide range of future problems. Here and at
              many other points, hierarchical reinforcement learning offers an
              appealing framework for investigating the computational and
              neural underpinnings of hierarchically structured behavior.",
  journal  = "Cognition",
  volume   =  113,
  number   =  3,
  pages    = "262--280",
  month    =  dec,
  year     =  2009,
  language = "en"
}


@ARTICLE{Yamaguchi2018-xp,
  title    = "Identification of animal behavioral strategies by inverse
              reinforcement learning",
  author   = "Yamaguchi, Shoichiro and Naoki, Honda and Ikeda, Muneki and
              Tsukada, Yuki and Nakano, Shunji and Mori, Ikue and Ishii, Shin",
  abstract = "Animals are able to reach a desired state in an environment by
              controlling various behavioral patterns. Identification of the
              behavioral strategy used for this control is important for
              understanding animals' decision-making and is fundamental to
              dissect information processing done by the nervous system.
              However, methods for quantifying such behavioral strategies have
              not been fully established. In this study, we developed an
              inverse reinforcement-learning (IRL) framework to identify an
              animal's behavioral strategy from behavioral time-series data. We
              applied this framework to C. elegans thermotactic behavior; after
              cultivation at a constant temperature with or without food, fed
              worms prefer, while starved worms avoid the cultivation
              temperature on a thermal gradient. Our IRL approach revealed that
              the fed worms used both the absolute temperature and its temporal
              derivative and that their behavior involved two strategies:
              directed migration (DM) and isothermal migration (IM). With DM,
              worms efficiently reached specific temperatures, which explains
              their thermotactic behavior when fed. With IM, worms moved along
              a constant temperature, which reflects isothermal tracking,
              well-observed in previous studies. In contrast to fed animals,
              starved worms escaped the cultivation temperature using only the
              absolute, but not the temporal derivative of temperature. We also
              investigated the neural basis underlying these strategies, by
              applying our method to thermosensory neuron-deficient worms.
              Thus, our IRL-based approach is useful in identifying animal
              strategies from behavioral time-series data and could be applied
              to a wide range of behavioral studies, including decision-making,
              in other organisms.",
  journal  = "PLoS Comput. Biol.",
  volume   =  14,
  number   =  5,
  pages    = "e1006122",
  month    =  may,
  year     =  2018,
  language = "en"
}
