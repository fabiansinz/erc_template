@ARTICLE{Parker2020-jz,
  title    = "{Movement-Related} Signals in Sensory Areas: Roles in Natural
              Behavior",
  author   = "Parker, Philip R L and Brown, Morgan A and Smear, Matthew C and
              Niell, Cristopher M",
  abstract = "Recent studies have demonstrated prominent and widespread
              movement-related signals in the brain of head-fixed mice, even in
              primary sensory areas. However, it is still unknown what role
              these signals play in sensory processing. Why are these sensory
              areas `contaminated' by movement signals? During natural
              behavior, animals actively acquire sensory information as they
              move through the environment and use this information to guide
              ongoing actions. In this context, movement-related signals could
              allow sensory systems to predict self-induced sensory changes and
              extract additional information about the environment. In this
              review we summarize recent findings on the presence of
              movement-related signals in sensory areas and discuss how their
              study, in the context of natural freely moving behaviors, could
              advance models of sensory processing.",
  journal  = "Trends Neurosci.",
  year     =  2020,
  keywords = "unread;read next;behavior;ERC Consolidator 2023"
}

@ARTICLE{Rezende2015-mx,
  title    = "Variational Inference with Normalizing Flows",
  author   = "Rezende, Danilo Jimenez and Mohamed, Shakir",
  abstract = "The choice of approximate posterior distribution is one of the
              core problems in variational inference. Most applications of
              variational inference employ simple families of posterior
              approximations in order to allow for efficient inference,
              focusing on mean-field or other simple structured approximations.
              This restriction has a significant impact on the quality of
              inferences made using variational methods. We introduce a new
              approach for specifying flexible, arbitrarily complex and
              scalable approximate posterior distributions. Our approximations
              are distributions constructed through a normalizing flow, whereby
              a simple initial density is transformed into a more complex one
              by applying a sequence of invertible transformations until a
              desired level of complexity is attained. We use this view of
              normalizing flows to develop categories of finite and
              infinitesimal flows and provide a unified view of approaches for
              constructing rich posterior approximations. We demonstrate that
              the theoretical advantages of having posteriors that better match
              the true posterior, combined with the scalability of amortized
              variational approaches, provides a clear improvement in
              performance and applicability of variational inference.",
  journal  = "1505.05770",
  year     =  2015,
  keywords = "unread;read next;probabilistic networks;2021 Bashiri Latent
              Flow;ERC Consolidator 2023"
}

@ARTICLE{Voigts2019-sv,
  title    = "Somatic and Dendritic Encoding of Spatial Variables in
              Retrosplenial Cortex Differs during {2D} Navigation",
  author   = "Voigts, Jakob and Harnett, Mark T",
  abstract = "Active amplification of organized synaptic inputs in dendrites
              can endow individual neurons with the ability to perform complex
              computations. However, whether dendrites in behaving animals
              perform independent local computations is not known. Such
              activity may be particularly important for complex behavior,
              where neurons integrate multiple streams of information.
              Head-restrained imaging has yielded important insights into
              cellular and circuit function, but this approach limits behavior
              and the underlying computations. We describe a method for
              full-featured 2-photon imaging in awake mice during free
              locomotion with volitional head rotation. We examine head
              direction and position encoding in simultaneously imaged apical
              tuft dendrites and their respective cell bodies in retrosplenial
              cortex, an area that encodes multi-modal navigational
              information. Activity in dendrites was not determined solely by
              somatic activity but reflected distinct navigational variables,
              fulfilling the requirements for dendritic computation. Our
              approach provides a foundation for studying sub-cellular
              processes during complex behaviors.",
  journal  = "Neuron",
  year     =  2019,
  keywords = "unread;4 topics;ERC Consolidator 2023"
}

@ARTICLE{Semedo2019-ll,
  title    = "Cortical Areas Interact through a Communication Subspace",
  author   = "Semedo, Jo{\~a}o D and Zandvakili, Amin and Machens, Christian K
              and Yu, Byron M and Kohn, Adam",
  abstract = "Most brain functions involve interactions among multiple,
              distinct areas or nuclei. For instance, visual processing in
              primates requires the appropriate relaying of signals across many
              distinct cortical areas. Yet our understanding of how populations
              of neurons in interconnected brain areas communicate is in its
              infancy. Here we investigate how trial-to-trial fluctuations of
              population responses in primary visual cortex (V1) are related to
              simultaneously recorded population responses in area V2. Using
              dimensionality reduction methods, we find that V1-V2 interactions
              occur through a communication subspace: V2 fluctuations are
              related to a small subset of V1 population activity patterns,
              distinct from the largest fluctuations shared among neurons
              within V1. In contrast, interactions between subpopulations
              within V1 are less selective. We propose that the communication
              subspace may be a general, population-level mechanism by which
              activity can be selectively routed across brain areas.",
  journal  = "Neuron",
  volume   =  102,
  number   =  1,
  pages    = "249--259.e4",
  year     =  2019,
  keywords = "unread;primate vision;ERC Consolidator 2023"
}

@ARTICLE{Tang2018-gn,
  title    = "Recurrent computations for visual pattern completion",
  author   = "Tang, Hanlin and Schrimpf, Martin and Lotter, William and
              Moerman, Charlotte and Paredes, Ana and Caro, Josue Ortega and
              Hardesty, Walter and Cox, David and Kreiman, Gabriel and Ortega,
              Josue",
  abstract = "Making inferences from partial information constitutes a critical
              aspect of cognition. During visual perception, pattern completion
              enables recognition of poorly visible or occluded objects. We
              combined psychophysics, physiology, and computational models to
              test the hypothesis that pattern completion is implemented by
              recurrent computations and present three pieces of evidence that
              are consistent with this hypothesis. First, subjects robustly
              recognized objects even when they were rendered <15\% visible,
              but recognition was largely impaired when processing was
              interrupted by backward masking. Second, invasive physiological
              responses along the human ventral cortex exhibited visually
              selective responses to partially visible objects that were
              delayed compared with whole objects, suggesting the need for
              additional computations. These physiological delays were
              correlated with the effects of backward masking. Third,
              state-of-the-art feed-forward computational architectures were
              not robust to partial visibility. However, recognition
              performance was recovered when the model was augmented with
              attractor-based recurrent connectivity. The recurrent model was
              able to predict which images of heavily occluded objects were
              easier or harder for humans to recognize, could capture the
              effect of introducing a backward mask on recognition behavior,
              and was consistent with the physiological delays along the human
              ventral visual stream. These results provide a strong argument of
              plausibility for the role of recurrent computations in making
              visual inferences from partial information.",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  115,
  number   =  35,
  pages    = "201719397",
  year     =  2018,
  keywords = "unread;man, brains and machines;ERC Consolidator 2023"
}

@ARTICLE{Mineault2016-fk,
  title    = "Enhanced Spatial Resolution During Locomotion and Heightened
              Attention in Mouse Primary Visual Cortex",
  author   = "Mineault, Patrick J and Tring, Elaine and Trachtenberg, Joshua T
              and Ringach, Dario L",
  abstract = "We do not fully understand how behavioral state modulates the
              processing and transmission of sensory signals. Here, we studied
              the cortical representation of the retinal image in mice that
              spontaneously switched between a state of rest and a constricted
              pupil, and one of active locomotion and a dilated pupil,
              indicative of heightened attention. We measured the selectivity
              of neurons in primary visual cortex for orientation and spatial
              frequency, as well as their response gain, in these two
              behavioral states. Consistent with prior studies, we found that
              preferred orientation and spatial frequency remained invariant
              across states, whereas response gain increased during locomotion
              relative to rest. Surprisingly, relative gain, defined as the
              ratio between the gain during locomotion and the gain during
              rest, was not uniform across the population. Cells tuned to high
              spatial frequencies showed larger relative gain compared with
              those tuned to lower spatial frequencies. The preferential
              enhancement of high-spatial-frequency information was also
              reflected in our ability to decode the stimulus from population
              activity. Finally, we show that changes in gain originate from
              shifts in the operating point of neurons along a spiking
              nonlinearity as a function of behavioral state. Differences in
              the relative gain experienced by neurons with high and low
              spatial frequencies are due to corresponding differences in how
              these cells shift their operating points between behavioral
              states. SIGNIFICANCE STATEMENT How behavioral state modulates the
              processing and transmission of sensory signals remains poorly
              understood. Here, we show that the mean firing rate and neuronal
              gain increase during locomotion as a result in a shift of the
              operating point of neurons. We define relative gain as the ratio
              between the gain of neurons during locomotion and rest.
              Interestingly, relative gain is higher in cells with preferences
              for higher spatial frequencies than those with
              low-spatial-frequency selectivity. This means that, during a
              state of locomotion and heightened attention, the population
              activity in primary visual cortex can support better spatial
              acuity, a phenomenon that parallels the improved spatial
              resolution observed in human subjects during the allocation of
              spatial attention.",
  journal  = "J. Neurosci.",
  volume   =  36,
  number   =  24,
  pages    = "6382--6392",
  year     =  2016,
  keywords = "unread;read next;rodent vision;ERC Consolidator 2023"
}

@ARTICLE{Kreiman2020-rq,
  title    = "Beyond the feedforward sweep: feedback computations in the visual
              cortex",
  author   = "Kreiman, Gabriel and Serre, Thomas",
  abstract = "Visual perception involves the rapid formation of a coarse image
              representation at the onset of visual processing, which is
              iteratively refined by late computational processes. These early
              versus late time windows approximately map onto feedforward and
              feedback processes, respectively. State-of-the-art convolutional
              neural networks, the main engine behind recent machine vision
              successes, are feedforward architectures. Their successes and
              limitations provide critical information regarding which visual
              tasks can be solved by purely feedforward processes and which
              require feedback mechanisms. We provide an overview of recent
              work in cognitive neuroscience and machine vision that highlights
              the possible role of feedback processes for both visual
              recognition and beyond. We conclude by discussing important open
              questions for future research.",
  journal  = "Ann. N. Y. Acad. Sci.",
  volume   =  1464,
  number   =  1,
  pages    = "222--241",
  year     =  2020,
  keywords = "unread;man, brains and machines;ERC Consolidator 2023"
}

@ARTICLE{Saleem2013-fx,
  title    = "Integration of visual motion and locomotion in mouse visual
              cortex",
  author   = "Saleem, Aman B and Ayaz, Asl{\i} Asli and Jeffery, Kathryn J and
              Harris, Kenneth D and Carandini, Matteo",
  abstract = "The primary visual cortex (V1) carries signals related to visual
              speed, and its responses are also affected by run speed. Here the
              authors report that nearly half of the V1 neurons were reliably
              driven by combinations of visual speed and run speed. As a
              population, V1 neurons predicted a linear combination of visual
              and run speed better than visual or run speeds alone. Successful
              navigation through the world requires accurate estimation of
              one's own speed. To derive this estimate, animals integrate
              visual speed gauged from optic flow and run speed gauged from
              proprioceptive and locomotor systems. The primary visual cortex
              (V1) carries signals related to visual speed, and its responses
              are also affected by run speed. To study how V1 combines these
              signals during navigation, we recorded from mice that traversed a
              virtual environment. Nearly half of the V1 neurons were reliably
              driven by combinations of visual speed and run speed. These
              neurons performed a weighted sum of the two speeds. The weights
              were diverse across neurons, and typically positive. As a
              population, V1 neurons predicted a linear combination of visual
              and run speeds better than either visual or run speeds alone.
              These data indicate that V1 in the mouse participates in a
              multimodal processing system that integrates visual motion and
              locomotion during navigation.",
  journal  = "Nat. Neurosci.",
  volume   =  16,
  number   =  12,
  pages    = "1864--1869",
  year     =  2013,
  keywords = "unread;rodent vision;brain state;ERC Consolidator 2023"
}

@ARTICLE{Singer2018-sf,
  title    = "Sensory cortex is optimised for prediction of future input",
  author   = "Singer, Y and Teramoto, Y and Willmore, B D B and King, A J and
              Schnupp, J W H and Harper, N S",
  abstract = "Neurons in sensory cortex are tuned to diverse features in
              natural scenes. But what determines which features neurons become
              selective to? Here we explore the idea that neuronal selectivity
              is optimised to represent features in the recent sensory past
              that best predict immediate future inputs. We tested this
              hypothesis using simple feedforward neural networks, which were
              trained to predict the next few video or audio frames in clips of
              natural scenes. The networks developed receptive fields that
              closely matched those of real cortical neurons in different
              mammalian species, including the oriented spatial tuning of
              primary visual cortex, the frequency selectivity of primary
              auditory cortex and, most notably, their temporal tuning
              properties. Furthermore, the better a network predicted future
              inputs the more closely its receptive fields resembled those in
              the brain. This suggests that sensory processing is optimised to
              extract those features with the most capacity to predict future
              input.",
  journal  = "Elife",
  volume   =  7,
  pages    = "e31557",
  year     =  2018,
  keywords = "unread;normative theories;ERC Consolidator 2023"
}

@ARTICLE{Kao2015-aa,
  title     = "Single-trial dynamics of motor cortex and their applications to
               brain-machine interfaces",
  author    = "Kao, Jonathan C and Nuyujukian, Paul and Ryu, Stephen I and
               Churchland, Mark M and Cunningham, John P and Shenoy, Krishna V",
  abstract  = "Increasing evidence suggests that neural population responses
               have their own internal drive, or dynamics, that describe how
               the neural population evolves through time. An important
               prediction of neural dynamical models is that previously
               observed neural activity is informative of noisy
               yet-to-be-observed activity on single-trials, and may thus have
               a denoising effect. To investigate this prediction, we built and
               characterized dynamical models of single-trial motor cortical
               activity. We find these models capture salient dynamical
               features of the neural population and are informative of future
               neural activity on single trials. To assess how neural dynamics
               may beneficially denoise single-trial neural activity, we
               incorporate neural dynamics into a brain-machine interface
               (BMI). In online experiments, we find that a neural dynamical
               BMI achieves substantially higher performance than its
               non-dynamical counterpart. These results provide evidence that
               neural dynamics beneficially inform the temporal evolution of
               neural activity on single trials and may directly impact the
               performance of BMIs.",
  journal   = "Nat. Commun.",
  publisher = "Nature Publishing Group",
  volume    =  6,
  number    = "May",
  year      =  2015,
  keywords  = "unread;brain state;ERC Consolidator 2023"
}

@ARTICLE{Froudarakis2019-yt,
  title    = "The Visual Cortex in Context",
  author   = "Froudarakis, Emmanouil and Fahey, Paul G and Reimer, Jacob and
              Smirnakis, Stelios M and Tehovnik, Edward J and Tolias, Andreas S",
  abstract = "In this article, we review the anatomical inputs and outputs to
              the mouse primary visual cortex, area V1. Our survey of data from
              the Allen Institute Mouse Connectivity project indicates that
              mouse V1 is highly interconnected with both cortical and
              subcortical brain areas. This pattern of innervation allows for
              computations that depend on the state of the animal and on
              behavioral goals, which contrasts with simple feedforward,
              hierarchical models of visual processing. Thus, to have an
              accurate description of the function of V1 during mouse behavior,
              its involvement with the rest of the brain circuitry has to be
              considered. Finally, it remains an open question whether the
              primary visual cortex of higher mammals displays the same degree
              of sensorimotor integration in the early visual system.",
  journal  = "Annual Review of Vision Science",
  volume   =  5,
  pages    = "317--339",
  year     =  2019,
  keywords = "V1 circuits; behavioral state; corticofugal projections;
              corticopetal projections; sensorimotor control; transcortical
              projections;unread;read next;4 behavior;4 mouse;rodent vision;ERC
              Consolidator 2023"
}

@ARTICLE{Froudarakis2020-xh,
  title    = "Object manifold geometry across the mouse cortical visual
              hierarchy",
  author   = "Froudarakis, Emmanouil and Cohen, Uri and Diamantaki, Maria and
              Walker, Edgar Y and Reimer, Jacob and Berens, Philipp and
              Sompolinsky, Haim and Tolias, Andreas S",
  abstract = "Despite variations in appearance we robustly recognize objects.
              Neuronal populations responding to objects presented under
              varying conditions form object manifolds and hierarchically
              organized visual areas are thought to untangle pixel intensities
              into linearly decodable object representations. However, the
              associated changes in the geometry of object manifolds along the
              cortex remain unknown. Using home cage training we showed that
              mice are capable of invariant object recognition. We
              simultaneously recorded the responses of thousands of neurons to
              measure the information about object identity available across
              the visual cortex and found that lateral visual areas LM, LI and
              AL carry more linearly decodable object identity information
              compared to other visual areas. We applied the theory of linear
              separability of manifolds, and found that the increase in
              classification capacity is associated with a decrease in the
              dimension and radius of the object manifold, identifying features
              of the population code that enable invariant object coding.",
  journal  = "bioRxiv",
  pages    = "2020.08.20.258798",
  year     =  2020,
  keywords = "unread;rodent vision;ERC Consolidator 2023"
}

@ARTICLE{Mathis2018-lk,
  title     = "{DeepLabCut}: markerless pose estimation of user-defined body
               parts with deep learning",
  author    = "Mathis, A and Mamidanna, P and Cury, K M and Abe, T and Murthy,
               V N and Mathis, M W and Bethge, M",
  journal   = "Nat. Neurosci.",
  publisher = "Springer US",
  volume    =  21,
  number    =  9,
  pages     = "1281--1289",
  year      =  2018,
  keywords  = "\_imported;ERC Consolidator 2023"
}

@ARTICLE{Shi_undated-cq,
  title    = "A Convolutional Network Architecture Driven by Mouse
              Neuroanatomical Data",
  author   = "Shi, Jianghong and Buice, Michael A and Shea-Brown, Eric and
              Mihalas, Stefan and Tripp, Bryan",
  keywords = "unread;inductive bias;rodent vision;ERC Consolidator 2023"
}

@ARTICLE{Erisken2014-un,
  title    = "Effects of locomotion extend throughout the mouse early visual
              system",
  author   = "Erisken, S and Vaiceliunaite, A and Jurjut, O and Fiorini, M and
              Katzner, S and Busse, L",
  abstract = "Background Neural responses in visual cortex depend not only on
              sensory input but also on behavioral context. One such context is
              locomotion, which modulates single-neuron activity in primary
              visual cortex (V1). How locomotion affects neuronal populations
              across cortical layers and in precortical structures is not well
              understood. Results We performed extracellular multielectrode
              recordings in the visual system of mice during locomotion and
              stationary periods. We found that locomotion influenced activity
              of V1 neurons with a characteristic laminar profile and shaped
              the population response by reducing pairwise correlations.
              Although the reduction of pairwise correlations was restricted to
              cortex, locomotion slightly but consistently increased firing
              rates and controlled tuning selectivity already in the
              dorsolateral geniculate nucleus (dLGN) of the thalamus. At the
              level of the eye, increases in locomotion speed were associated
              with pupil dilation. Conclusions These findings document further,
              nonmultiplicative effects of locomotion, reaching earlier
              processing stages than cortex.",
  journal  = "Curr. Biol.",
  volume   =  24,
  number   =  24,
  pages    = "2899--2907",
  year     =  2014,
  keywords = "netgard;ERC Consolidator 2023"
}

@BOOK{Dadarlat2017-jw,
  title    = "Locomotion Enhances Neural Encoding of Visual Stimuli in Mouse
              {V1}",
  author   = "Dadarlat, Maria C and Stryker, Michael P",
  abstract = "Neurons in mouse primary visual cortex (V1) are selective for
              particular properties of visual stimuli. Locomotion causes a
              change in cortical state that leaves their selectivity unchanged
              but strengthens their responses. Both locomotion and the change
              in cortical state are thought to be initiated by projections from
              the mesencephalic locomotor region, the latter through a
              disinhibitory circuit in V1. By recording simultaneously from a
              large number of single neurons in alert mice viewing moving
              gratings, we investigated the relationship between locomotion and
              the information contained within the neural population. We found
              that locomotion improved encoding of visual stimuli in V1 by two
              mechanisms. First, locomotion-induced increases in firing rates
              enhanced the mutual information between visual stimuli and single
              neuron responses over a fixed window of time. Second, stimulus
              discriminability was improved, even for fixed population firing
              rates, because of a decrease in noise correlations across the
              population. These two mechanisms contributed differently to
              improvements in discriminability across cortical layers, with
              changes in firing rates most important in the upper layers and
              changes in noise correlations most important in layer V.
              Together, these changes resulted in a threefold to fivefold
              reduction in the time needed to precisely encode grating
              direction and orientation. These results support the hypothesis
              that cortical state shifts during locomotion to accommodate an
              increased load on the visual system when mice are
              moving.SIGNIFICANCE STATEMENT This paper contains three novel
              findings about the representation of information in neurons
              within the primary visual cortex of the mouse. First, we show
              that locomotion reduces by at least a factor of 3 the time needed
              for information to accumulate in the visual cortex that allows
              the distinction of different visual stimuli. Second, we show that
              the effect of locomotion is to increase information in cells of
              all layers of the visual cortex. Third, we show that the means by
              which information is enhanced by locomotion differs between the
              upper layers, where the major effect is the increasing of firing
              rates, and in layer V, where the major effect is the reduction in
              noise correlations.",
  volume   =  37,
  pages    = "3764--3775",
  year     =  2017,
  keywords = "read;behavior;brain state;color MEI;ERC Consolidator 2023"
}

@ARTICLE{Clancy2019-ta,
  title     = "Locomotion-dependent remapping of distributed cortical networks",
  author    = "Clancy, Kelly B and Orsolic, Ivana and Mrsic-Flogel, Thomas D",
  abstract  = "The interactions between neocortical areas are fluid and
               state-dependent, but how individual neurons couple to
               cortex-wide network dynamics remains poorly understood. We
               correlated the spiking of neurons in primary visual (V1) and
               retrosplenial (RSP) cortex to activity across dorsal cortex,
               recorded simultaneously by widefield calcium imaging. Neurons
               were correlated with distinct and reproducible patterns of
               activity across the cortical surface; while some fired
               predominantly with their local area, others coupled to activity
               in distal areas. The extent of distal coupling was predicted by
               how strongly neurons correlated with the local network. Changes
               in brain state triggered by locomotion strengthened affiliations
               of V1 neurons with higher visual and motor areas, while
               strengthening distal affiliations of RSP neurons with sensory
               cortices. Thus, the diverse coupling of individual neurons to
               cortex-wide activity patterns is restructured by running in an
               area-specific manner, resulting in a shift in the mode of
               cortical processing during locomotion.",
  journal   = "Nat. Neurosci.",
  publisher = "Springer US",
  year      =  2019,
  keywords  = "unread;brain state;ERC Consolidator 2023"
}

@ARTICLE{Niell2010-bs,
  title     = "Modulation of Visual Responses by Behavioral State in Mouse
               Visual Cortex",
  author    = "Niell, Cristopher M and Stryker, Michael P",
  abstract  = "Studies of visual processing in rodents have conventionally been
               performed on anesthetized animals, precluding examination of the
               effects of behavior on visually evoked responses. We have now
               studied the response properties of neurons in primary visual
               cortex of awake mice that were allowed to run on a freely
               rotating spherical treadmill with their heads fixed. Most
               neurons showed more than a doubling of visually evoked firing
               rate as the animal transitioned from standing still to running,
               without changes in spontaneous firing or stimulus selectivity.
               Tuning properties in the awake animal were similar to those
               measured previously in anesthetized animals. Response magnitude
               in the lateral geniculate nucleus did not increase with
               locomotion, demonstrating that the striking change in
               responsiveness did not result from peripheral effects at the
               eye. Interestingly, some narrow-spiking cells were spontaneously
               active during running but suppressed by visual stimuli. These
               results demonstrate powerful cell-type-specific modulation of
               visual processing by behavioral state in awake mice.
               \copyright{} 2010 Elsevier Inc. All rights reserved.",
  journal   = "Neuron",
  publisher = "Elsevier Ltd",
  volume    =  65,
  number    =  4,
  pages     = "472--479",
  year      =  2010,
  keywords  = "SYSNEURO;system identification;brain state;SFB 1456 Mathematic
               of Experiment;ERC Consolidator 2023"
}

@ARTICLE{Stringer2019-lt,
  title    = "Spontaneous behaviors drive multidimensional, brainwide activity",
  author   = "Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas
              and Reddy, Charu Bai and Carandini, Matteo and Harris, Kenneth D",
  abstract = "Neuronal populations in sensory cortex produce variable responses
              to sensory stimuli and exhibit intricate spontaneous activity
              even without external sensory input. Cortical variability and
              spontaneous activity have been variously proposed to represent
              random noise, recall of prior experience, or encoding of ongoing
              behavioral and cognitive variables. Recording more than 10,000
              neurons in mouse visual cortex, we observed that spontaneous
              activity reliably encoded a high-dimensional latent state, which
              was partially related to the mouse's ongoing behavior and was
              represented not just in visual cortex but also across the
              forebrain. Sensory inputs did not interrupt this ongoing signal
              but added onto it a representation of external stimuli in
              orthogonal dimensions. Thus, visual cortical population activity,
              despite its apparently noisy structure, reliably encodes an
              orthogonal fusion of sensory and multidimensional behavioral
              information.",
  journal  = "Science",
  volume   =  364,
  number   =  6437,
  year     =  2019,
  keywords = "4 behavior;4 mouse;read next;unread;brain state;SFB 1456
              Mathematic of Experiment;ERC Consolidator 2023"
}

@ARTICLE{Christensen2017-bx,
  title    = "Running reduces firing but improves coding in rodent higher-order
              visual cortex",
  author   = "Christensen, Amelia J and Pillow, Jonathan W",
  abstract = "Running profoundly alters stimulus-response properties in mouse
              primary visual cortex (V1), but its effects in higher-order
              visual cortex remain unknown. Here we systematically investigated
              how locomotion modulates visual responses across six visual areas
              and three cortical layers using a massive dataset from the Allen
              Brain Institute. Although running has been shown to increase
              firing in V1, we found that it suppressed firing in higher-order
              visual areas. Despite this reduction in gain, visual responses
              during running could be decoded more accurately than visual
              responses during stationary periods. We show that this effect was
              not attributable to changes in noise correlations, and propose
              that it instead arises from increased reliability of single
              neuron responses during running.",
  journal  = "bioRxiv",
  year     =  2017,
  keywords = "Anatomy; Artificial intelligence; Bioinformatics; Biology;
              Neuron; Pattern recognition; Visual cortex;brain state;ERC
              Consolidator 2023"
}

@ARTICLE{Guclu2015-hv,
  title    = "Deep Neural Networks Reveal a Gradient in the Complexity of
              Neural Representations across the Ventral Stream",
  author   = "G{\"u}{\c c}l{\"u}, Umut and van Gerven, Marcel A J",
  abstract = "Converging evidence suggests that the primate ventral visual
              pathway encodes increasingly complex stimulus features in
              downstream areas. We quantitatively show that there indeed exists
              an explicit gradient for feature complexity in the ventral
              pathway of the human brain. This was achieved by mapping
              thousands of stimulus features of increasing complexity across
              the cortical sheet using a deep neural network. Our approach also
              revealed a fine-grained functional specialization of downstream
              areas of the ventral stream. Furthermore, it allowed decoding of
              representations from human brain activity at an unsurpassed
              degree of accuracy, confirming the quality of the developed
              approach. Stimulus features that successfully explained neural
              responses indicate that population receptive fields were
              explicitly tuned for object categorization. This provides strong
              support for the hypothesis that object categorization is a
              guiding principle in the functional organization of the primate
              ventral stream.",
  journal  = "J. Neurosci.",
  volume   =  35,
  number   =  27,
  pages    = "10005--10014",
  year     =  2015,
  keywords = "ERC Consolidator 2023"
}

@INCOLLECTION{Sinz2018-sk,
  title     = "Stimulus domain transfer in recurrent models for large scale
               cortical population prediction on video",
  booktitle = "Advances in Neural Information Processing Systems 31",
  author    = "Sinz, F and Ecker, A S and Fahey, P and Walker, E and Cobos, E
               and Froudarakis, E and Yatsenko, D and Pitkow, X and Reimer, J
               and Tolias, A",
  abstract  = "To better understand the representations in visual cortex, we
               need to generate better predictions of neural activity in awake
               animals presented with their ecological input: natural video.
               Despite recent advances in models for static images, models for
               predicting responses to natural video are scarce and standard
               linear-nonlinear models perform poorly. We developed a new deep
               recurrent network architecture that predicts inferred spiking
               activity of thousands of mouse V1 neurons simultaneously
               recorded with two-photon microscopy, while accounting for
               confounding factors such as the animal's gaze position and brain
               state changes related to running state and pupil dilation.
               Powerful system identification models provide an opportunity to
               gain insight into cortical functions through in silico
               experiments that can subsequently be tested in the brain.
               However, in many cases this approach requires that the model is
               able to generalize to stimulus statistics that it was not
               trained on, such as band-limited noise and other parameterized
               stimuli. We investigated these domain transfer properties in our
               model and find that our model trained on natural images is able
               to correctly predict the orientation tuning of neurons in
               responses to artificial noise stimuli. Finally, we show that we
               can fully generalize from movies to noise and maintain high
               predictive performance on both stimulus domains by fine-tuning
               only the final layer's weights on a network otherwise trained on
               natural movies. The converse, however, is not true.",
  year      =  2018,
  keywords  = "read;\_imported;2021 Reconstruction;SFB 1456 Mathematic of
               Experiment;ERC Consolidator 2023"
}

@ARTICLE{Walker2019-yw,
  title     = "Inception loops discover what excites neurons most using deep
               predictive models",
  author    = "Walker, E Y and Sinz, F H and Cobos, E and Muhammad, T and
               Froudarakis, E and Fahey, P G and Ecker, A S and Reimer, J and
               Pitkow, X and Tolias, A S",
  abstract  = "Finding sensory stimuli that drive neurons optimally is central
               to understanding information processing in the brain. However,
               optimizing sensory input is difficult due to the predominantly
               nonlinear nature of sensory processing and high dimensionality
               of the input. We developed `inception loops', a closed-loop
               experimental paradigm combining in vivo recordings from
               thousands of neurons with in silico nonlinear response modeling.
               Our end-to-end trained, deep-learning-based model predicted
               thousands of neuronal responses to arbitrary, new natural input
               with high accuracy and was used to synthesize optimal
               stimuli---most exciting inputs (MEIs). For mouse primary visual
               cortex (V1), MEIs exhibited complex spatial features that
               occurred frequently in natural scenes but deviated strikingly
               from the common notion that Gabor-like stimuli are optimal for
               V1. When presented back to the same neurons in vivo, MEIs drove
               responses significantly better than control stimuli. Inception
               loops represent a widely applicable technique for dissecting the
               neural mechanisms of sensation.",
  journal   = "Nat. Neurosci.",
  publisher = "Springer US",
  year      =  2019,
  keywords  = "\_imported;CRCNS;2021 Reconstruction;U24;SFB 1456 Mathematic of
               Experiment;ERC Consolidator 2023"
}

@ARTICLE{Guclu2017-bi,
  title     = "Increasingly complex representations of natural movies across
               the dorsal stream are shared between subjects",
  author    = "G{\"u}{\c c}l{\"u}, U and van Gerven, M A J",
  abstract  = "Recently, deep neural networks (DNNs) have been shown to provide
               accurate predictions of neural responses across the ventral
               visual pathway. We here explore whether they also provide
               accurate predictions of neural responses across the dorsal
               visual pathway, which is thought to be devoted to motion
               processing and action recognition. This is achieved by training
               deep neural networks to recognize actions in videos and
               subsequently using them to predict neural responses while
               subjects are watching natural movies. Moreover, we explore
               whether dorsal stream representations are shared between
               subjects. In order to address this question, we examine if
               individual subject predictions can be made in a common
               representational space estimated via hyperalignment. Results
               show that a DNN trained for action recognition can be used to
               accurately predict how dorsal stream responds to natural movies,
               revealing a correspondence in representations of DNN layers and
               dorsal stream areas. It is also demonstrated that models
               operating in a common representational space can generalize to
               responses of multiple or even unseen individual subjects to
               novel spatio-temporal stimuli in both encoding and decoding
               settings, suggesting that a common representational space
               underlies dorsal stream responses across multiple subjects.",
  journal   = "Neuroimage",
  publisher = "Elsevier Inc.",
  volume    =  145,
  pages     = "329--336",
  year      =  2017,
  keywords  = "Decoding; Deep neural network; Dorsal stream; Encoding;
               Hyperalignment;system identification;ERC Consolidator 2023"
}

@ARTICLE{Russell2019-dq,
  title    = "The influence of visual cortex on perception is modulated by
              behavioural state",
  author   = "Russell, Lloyd E and Yang, Zidan and Tan, Pei Lynn and Fi{\c
              s}ek, Mehmet and Packer, Adam M and Dalgleish, Henry W P and
              Chettih, Selmaan and Harvey, Christopher D and H{\"a}usser,
              Michael and Tan, Lynn Pei and Fi{\c s}ek, Mehmet and Packer, Adam
              M and Dalgleish, Henry W P and Chettih, Selmaan and Harvey,
              Christopher D and H{\"a}usser, Michael",
  abstract = "Our understanding of the link between neural activity and
              perception remains incomplete. Microstimulation and optogenetic
              experiments have shown that manipulating cortical activity can
              influence sensory-guided behaviour or elicit artificial percepts.
              And yet, some perceptual tasks can still be solved when sensory
              cortex is silenced or removed, suggesting that cortical activity
              may not always be essential. Reconciling these findings, and
              providing a quantitative framework linking cortical activity and
              behaviour, requires knowledge of the identity of the cells being
              activated during the behaviour, the engagement of the local and
              downstream networks, and the cortical and behavioural state.
              Here, we performed two-photon population calcium imaging in L2/3
              primary visual cortex (V1) of headfixed mice performing a visual
              detection task while simultaneously activating specific groups of
              neurons using targeted two-photon optogenetics during low
              contrast visual stimulation. Only activation of groups of cells
              with similar tuning to the relevant visual stimulus led to a
              measurable bias of detection behaviour. Targeted photostimulation
              revealed signatures of centre-surround, predominantly inhibitory
              and like-to-like connectivity motifs in the local network which
              shaped the visual stimulus representation and partially explained
              the change in stimulus detectability. Moreover, the behavioural
              effects depended on overall performance: when the task was
              challenging for the mouse, V1 activity was more closely linked to
              performance, and cortical stimulation boosted perception. In
              contrast, when the task was easy, V1 activity was less
              informative about performance and cortical stimulation suppressed
              stimulus detection. Altogether, we find that both the selective
              routing of information through functionally specific circuits,
              and the prevailing cortical state, make similarly large
              contributions to explaining the behavioural response to
              photostimulation. Our results thus help to reconcile
              contradictory findings about the involvement of primary sensory
              cortex in behavioural tasks, suggesting that the influence of
              cortical activity on behaviour is dynamically reassigned
              depending on the demands of the task.",
  journal  = "bioRxiv",
  pages    = "706010",
  year     =  2019,
  keywords = "unread;brain state;ERC Consolidator 2023"
}

@ARTICLE{Marshel2019-id,
  title    = "Cortical layer--specific critical dynamics triggering perception",
  author   = "Marshel, James H and Kim, Yoon Seok and Machado, Timothy A and
              Quirin, Sean and Benson, Brandon and Kadmon, Jonathan and Raja,
              Cephra and Chibukhchyan, Adelaida and Ramakrishnan, Charu and
              Inoue, Masatoshi and Shane, Janelle C and McKnight, Douglas J and
              Yoshizawa, Susumu and Kato, Hideaki E and Ganguli, Surya and
              Deisseroth, Karl",
  abstract = "Perceptual experiences may arise from neuronal activity patterns
              in mammalian neocortex. We probed mouse neocortex during visual
              discrimination using a red-shifted channelrhodopsin (ChRmine,
              discovered through structure-guided genome mining) alongside
              multiplexed multiphoton-holography (MultiSLM), achieving control
              of individually specified neurons spanning large cortical volumes
              with millisecond precision. Stimulating a critical number of
              stimulus-orientation-selective neurons drove widespread
              recruitment of functionally related neurons, a process enhanced
              by (but not requiring) orientation-discrimination task learning.
              Optogenetic targeting of orientation-selective ensembles elicited
              correct behavioral discrimination. Cortical layer--specific
              dynamics were apparent, as emergent neuronal activity
              asymmetrically propagated from layer 2/3 to layer 5, and smaller
              layer 5 ensembles were as effective as larger layer 2/3 ensembles
              in eliciting orientation discrimination behavior. Population
              dynamics emerging after optogenetic stimulation both correctly
              predicted behavior and resembled natural internal representations
              of visual stimuli at cellular resolution over volumes of cortex.",
  journal  = "Science",
  volume   =  365,
  number   =  6453,
  pages    = "eaaw5202",
  year     =  2019,
  keywords = "unread;primate vision;ERC Consolidator 2023"
}

@ARTICLE{Scarselli2009-gy,
  title    = "The graph neural network model",
  author   = "Scarselli, Franco and Gori, Marco and Tsoi, A C Ah Chung and
              Hagenbuchner, Markus and Monfardini, Gabriele",
  abstract = "Many underlying relationships among data in several areas of
              science and engineering, e.g., computer vision, molecular
              chemistry, molecular biology, pattern recognition, and data
              mining, can be represented in terms of graphs. In this paper, we
              propose a new neural network model, called graph neural network
              (GNN) model, that extends existing neural network methods for
              processing the data represented in graph domains. This GNN model,
              which can directly process most of the practically useful types
              of graphs, e.g., acyclic, cyclic, directed, and undirected,
              implements a function tau(G,n) is an element of IR(m) that maps a
              graph G and one of its nodes n into an m-dimensional Euclidean
              space. A supervised learning algorithm is derived to estimate the
              parameters of the proposed GNN model. The computational cost of
              the proposed algorithm is also considered. Some experimental
              results are shown to validate the proposed learning algorithm,
              and to demonstrate its generalization capabilities.",
  journal  = "IEEE Trans. Neural Netw.",
  volume   =  20,
  number   =  1,
  pages    = "61--80",
  year     =  2009,
  keywords = "4 Projects;netgard;SFB 1456 Mathematic of Experiment;ERC
              Consolidator 2023"
}

@ARTICLE{Yamins2016-ob,
  title    = "Using goal-driven deep learning models to understand sensory
              cortex",
  author   = "Yamins, Daniel L K and DiCarlo, James J",
  abstract = "Fueled by innovation in the computer vision and artificial
              intelligence communities, recent developments in computational
              neuroscience have used goal-driven hierarchical convolutional
              neural networks (HCNNs) to make strides in modeling neural
              single-unit and population responses in higher visual cortical
              areas. In this Perspective, we review the recent progress in a
              broader modeling context and describe some of the key technical
              innovations that have supported it. We then outline how the
              goal-driven HCNN approach can be used to delve even more deeply
              into understanding the development and organization of sensory
              cortical processing.",
  journal  = "Nat. Neurosci.",
  volume   =  19,
  number   =  3,
  pages    = "356--365",
  month    =  mar,
  year     =  2016,
  keywords = "read;microns;system identification;2021 Reconstruction;ERC
              Consolidator 2023",
  language = "en"
}

@INPROCEEDINGS{Lurz2020-ua,
  title      = "Generalization in data-driven models of primary visual cortex",
  booktitle  = "Proceedings of the International Conference for Learning
                Representations ({ICLR})",
  author     = "Lurz, Konstantin-Klemens and Bashiri, Mohammad and Willeke,
                Konstantin and Jagadish, Akshay K and Wang, Eric and Walker,
                Edgar Y and Cadena, Santiago A and Muhammad, Taliah and Cobos,
                Erick and Tolias, Andreas S and Ecker, Alexander S and Sinz,
                Fabian H",
  abstract   = "Deep neural networks (DNN) have set new standards at predicting
                responses of neural populations to visual input. Most such DNNs
                consist of a convolutional network (core) shared across all
                neurons which learns a representation of neural computation in
                visual cortex and a neuron-specific readout that linearly
                combines the relevant features in this representation. The goal
                of this paper is to test whether such a representation is
                indeed generally characteristic for visual cortex, i.e.
                generalizes between animals of a species, and what factors
                contribute to obtaining such a generalizing core. To push all
                non-linear computations into the core where the generalizing
                cortical features should be learned, we devise a novel readout
                that reduces the number of parameters per neuron in the readout
                by up to two orders of magnitude compared to the previous
                state-of-the-art. It does so by taking advantage of retinotopy
                and learns a Gaussian distribution over the neuron's receptive
                field position. With this new readout we train our network on
                neural responses from mouse primary visual cortex (V1) and
                obtain a gain in performance of 7\% compared to the previous
                state-of-the-art network. We then investigate whether the
                convolutional core indeed captures general cortical features by
                using the core in transfer learning to a different animal. When
                transferring a core trained on thousands of neurons from
                various animals and scans we exceed the performance of training
                directly on that animal by 12\%, and outperform a commonly used
                VGG16 core pre-trained on imagenet by 33\%. In addition,
                transfer learning with our data-driven core is more
                data-efficient than direct training, achieving the same
                performance with only 40\% of the data. Our model with its
                novel readout thus sets a new state-of-the-art for neural
                response prediction in mouse visual cortex from natural images,
                generalizes between animals, and captures better characteristic
                cortical features than current task-driven pre-training
                approaches such as VGG16. \#\#\# Competing Interest Statement
                The authors have declared no competing interest.",
  pages      = "2020.10.05.326256",
  month      =  oct,
  year       =  2020,
  keywords   = "own;CRCNS;SFB 1456 Mathematic of Experiment;ERC Consolidator
                2023",
  language   = "en",
  conference = "ICLR 2021"
}

@ARTICLE{Jin2020-wg,
  title    = "Mouse Higher Visual Areas Provide Both Distributed and
              Specialized Contributions to Visually Guided Behaviors",
  author   = "Jin, Miaomiao and Glickfeld, Lindsey L",
  abstract = "Cortical parallel processing streams segregate many diverse
              features of a sensory scene. However, some features are
              distributed across streams, begging the question of whether and
              how such distributed representations contribute to perception. We
              determined the necessity of the primary visual cortex (V1) and
              three key higher visual areas (lateromedial [LM], anterolateral
              [AL], and posteromedial [PM]) for perception of orientation and
              contrast, two features that are robustly encoded across all four
              areas. Suppressing V1, LM, or AL decreased sensitivity for both
              orientation discrimination and contrast detection, consistent
              with a role for these areas in sensory perception. In comparison,
              suppressing PM selectively increased false alarm (FA) rates
              during contrast detection, without any effect on orientation
              discrimination. This effect was not retinotopically specific,
              suggesting that suppression of PM altered sensory integration or
              the decision-making process rather than processing of local
              visual features. Thus, we find that distributed representations
              in the visual system can nonetheless support specialized
              perceptual roles for higher visual cortical areas.",
  journal  = "Curr. Biol.",
  volume   =  30,
  number   =  23,
  pages    = "4682--4692.e7",
  month    =  dec,
  year     =  2020,
  keywords = "contrast; d-prime; decision-making; false alarm; mouse visual
              cortex; optogenetics; orientation; psychophysics; sensory
              processing; speed;4 brainstate;4 mouse;read next;unread;ERC
              Consolidator 2023",
  language = "en"
}

@ARTICLE{Weichwald2015-ff,
  title    = "Causal interpretation rules for encoding and decoding models in
              neuroimaging",
  author   = "Weichwald, Sebastian and Meyer, Timm and {\"O}zdenizci, Ozan and
              Sch{\"o}lkopf, Bernhard and Ball, Tonio and Grosse-Wentrup,
              Moritz",
  abstract = "Causal terminology is often introduced in the interpretation of
              encoding and decoding models trained on neuroimaging data. In
              this article, we investigate which causal statements are
              warranted and which ones are not supported by empirical evidence.
              We argue that the distinction between encoding and decoding
              models is not sufficient for this purpose: relevant features in
              encoding and decoding models carry a different meaning in
              stimulus- and in response-based experimental paradigms.We show
              that only encoding models in the stimulus-based setting support
              unambiguous causal interpretations. By combining encoding and
              decoding models trained on the same data, however, we obtain
              insights into causal relations beyond those that are implied by
              each individual model type. We illustrate the empirical relevance
              of our theoretical findings on EEG data recorded during a
              visuo-motor learning task.",
  journal  = "Neuroimage",
  volume   =  110,
  pages    = "48--59",
  month    =  apr,
  year     =  2015,
  keywords = "Causal inference; Decoding models; Encoding models;
              Interpretation; Pattern recognition;read next;unread;ERC
              Consolidator 2023",
  language = "en"
}

@ARTICLE{Garrett2014-zm,
  title    = "Topography and areal organization of mouse visual cortex",
  author   = "Garrett, Marina E and Nauhaus, Ian and Marshel, James H and
              Callaway, Edward M",
  abstract = "To guide future experiments aimed at understanding the mouse
              visual system, it is essential that we have a solid handle on the
              global topography of visual cortical areas. Ideally, the method
              used to measure cortical topography is objective, robust, and
              simple enough to guide subsequent targeting of visual areas in
              each subject. We developed an automated method that uses
              retinotopic maps of mouse visual cortex obtained with intrinsic
              signal imaging (Schuett et al., 2002; Kalatsky and Stryker, 2003;
              Marshel et al., 2011) and applies an algorithm to automatically
              identify cortical regions that satisfy a set of quantifiable
              criteria for what constitutes a visual area. This approach
              facilitated detailed parcellation of mouse visual cortex,
              delineating nine known areas (primary visual cortex, lateromedial
              area, anterolateral area, rostrolateral area, anteromedial area,
              posteromedial area, laterointermediate area, posterior area, and
              postrhinal area), and revealing two additional areas that have
              not been previously described as visuotopically mapped in mice
              (laterolateral anterior area and medial area). Using the
              topographic maps and defined area boundaries from each animal, we
              characterized several features of map organization, including
              variability in area position, area size, visual field coverage,
              and cortical magnification. We demonstrate that higher areas in
              mice often have representations that are incomplete or biased
              toward particular regions of visual space, suggestive of
              specializations for processing specific types of information
              about the environment. This work provides a comprehensive
              description of mouse visuotopic organization and describes
              essential tools for accurate functional localization of visual
              areas.",
  journal  = "Journal of Neuroscience",
  volume   =  34,
  number   =  37,
  pages    = "12587--12600",
  month    =  sep,
  year     =  2014,
  keywords = "extrastriate; imaging; mouse; retinotopy; topography; visual
              cortex;4 brainstate;read next;unread;rodent vision;ERC
              Consolidator 2023",
  language = "en"
}

@UNPUBLISHED{Horrocks2021-re,
  title    = "Distinct neural dynamics underlie the encoding of visual speed in
              stationary and running mice",
  author   = "Horrocks, Edward A B and Saleem, Aman B",
  abstract = "Sensory experiences are often driven by an animal's self-motion
              and locomotion is known to modulate neural responses in the mouse
              visual system. This modulation is hypothesised to improve the
              processing of behaviourally relevant visual inputs, which may
              change rapidly during locomotion. However, little is known about
              how locomotion modulates the temporal dynamics (time courses) of
              visually-evoked neural responses. Here, we analysed the temporal
              dynamics of single neuron and population responses to dot field
              stimuli moving at a range of visual speeds using the Visual
              Coding dataset from the Allen Institute for Brain Science[1][1].
              Single neuron responses had diverse temporal dynamics that varied
              between stationary and running sessions. Increased dynamic range
              and more reliable responses in running sessions enabled faster,
              stronger and more persistent encoding of visual speed. Population
              activity reflected the temporal dynamics of single neuron
              responses, including their modulation by locomotor state - neural
              trajectories of population activity made more direct transitions
              between baseline and stimulus steady states in running sessions.
              The structure of population coding also changed with locomotor
              state -- population activity prioritised the encoding of visual
              speed in running, but not stationary sessions. Our results reveal
              a profound influence of locomotion on the temporal dynamics of
              neural responses. We demonstrate that during locomotion, mouse
              visual areas prioritise the encoding of potentially
              fast-changing, behaviourally relevant visual features. \#\#\#
              Competing Interest Statement The authors have declared no
              competing interest. [1]: \#ref-1",
  journal  = "bioRxiv",
  pages    = "2021.06.11.447904",
  month    =  jun,
  year     =  2021,
  keywords = "4 color MEI;read next;unread;ERC Consolidator 2023",
  language = "en"
}

@UNPUBLISHED{Luongo2021-ci,
  title    = "Mice and primates use distinct strategies for visual segmentation",
  author   = "Luongo, Francisco J and Liu, Lu and Ho, Chun Lum Andy and Hesse,
              Janis K and Wekselblatt, Joseph B and Lanfranchi, Francesco and
              Huber, Daniel and Tsao, Doris Y",
  abstract = "The rodent visual system has attracted great interest in recent
              years due to its experimental tractability, but the fundamental
              mechanisms used by the mouse to represent the visual world remain
              unclear. In the primate, researchers have argued from both
              behavioral and neural evidence that a key step in visual
              representation is ``figure-ground segmentation,'' the delineation
              of figures as distinct from backgrounds [[1][1]--[4][2]]. To
              determine if mice also show behavioral and neural signatures of
              figure-ground segmentation, we trained mice on a figure-ground
              segmentation task where figures were defined by gratings and
              naturalistic textures moving counterphase to the background.
              Unlike primates, mice were severely limited in their ability to
              segment figure from ground using the opponent motion cue, with
              segmentation behavior strongly dependent on the specific carrier
              pattern. Remarkably, when mice were forced to localize
              naturalistic patterns defined by opponent motion, they adopted a
              strategy of brute force memorization of texture patterns. In
              contrast, primates, including humans, macaques, and mouse lemurs,
              could readily segment figures independent of carrier pattern
              using the opponent motion cue. Consistent with mouse behavior,
              neural responses to the same stimuli recorded in mouse visual
              areas V1, RL, and LM also did not support texture-invariant
              segmentation of figures using opponent motion. Modeling revealed
              that the texture dependence of both the mouse's behavior and
              neural responses could be explained by a feedforward neural
              network lacking explicit segmentation capabilities. These
              findings reveal a fundamental limitation in the ability of mice
              to segment visual objects compared to primates. \#\#\# Competing
              Interest Statement The authors have declared no competing
              interest. [1]: \#ref-1 [2]: \#ref-4",
  journal  = "bioRxiv",
  pages    = "2021.07.04.451059",
  month    =  jul,
  year     =  2021,
  keywords = "4 mouse;read next;unread;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Luo2021-br,
  title     = "Architectures of neuronal circuits",
  author    = "Luo, Liqun",
  journal   = "Science",
  publisher = "American Association for the Advancement of Science (AAAS)",
  volume    =  373,
  number    =  6559,
  month     =  sep,
  year      =  2021,
  keywords  = "read next;unread;ERC Consolidator 2023",
  language  = "en"
}

@ARTICLE{Cisek_undated-kf,
  title    = "{BEYOND} {THE} {COMPUTER} {METAPHOR}: {BEHAVIOR} {AS}
              {INTERACTION}",
  author   = "Cisek, Paul",
  keywords = "unread;ERC Consolidator 2023"
}

@ARTICLE{Ho2016-jw,
  title         = "Generative Adversarial Imitation Learning",
  author        = "Ho, Jonathan and Ermon, Stefano",
  abstract      = "Consider learning a policy from example expert behavior,
                   without interaction with the expert or access to
                   reinforcement signal. One approach is to recover the
                   expert's cost function with inverse reinforcement learning,
                   then extract a policy from that cost function with
                   reinforcement learning. This approach is indirect and can be
                   slow. We propose a new general framework for directly
                   extracting a policy from data, as if it were obtained by
                   reinforcement learning following inverse reinforcement
                   learning. We show that a certain instantiation of our
                   framework draws an analogy between imitation learning and
                   generative adversarial networks, from which we derive a
                   model-free imitation learning algorithm that obtains
                   significant performance gains over existing model-free
                   methods in imitating complex behaviors in large,
                   high-dimensional environments.",
  month         =  jun,
  year          =  2016,
  keywords      = "4 behavior;read;ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1606.03476"
}

@ARTICLE{Whittington2021-rb,
  title         = "Relating transformers to models and neural representations
                   of the hippocampal formation",
  author        = "Whittington, James C R and Warren, Joseph and Behrens,
                   Timothy E J",
  abstract      = "Many deep neural network architectures loosely based on
                   brain networks have recently been shown to replicate neural
                   firing patterns observed in the brain. One of the most
                   exciting and promising novel architectures, the Transformer
                   neural network, was developed without the brain in mind. In
                   this work, we show that transformers, when equipped with
                   recurrent position encodings, replicate the precisely tuned
                   spatial representations of the hippocampal formation; most
                   notably place and grid cells. Furthermore, we show that this
                   result is no surprise since it is closely related to current
                   hippocampal models from neuroscience. We additionally show
                   the transformer version offers dramatic performance gains
                   over the neuroscience version. This work continues to bind
                   computations of artificial and brain networks, offers a
                   novel understanding of the hippocampal-cortical interaction,
                   and suggests how wider cortical areas may perform complex
                   tasks beyond current neuroscience models such as language
                   comprehension.",
  month         =  dec,
  year          =  2021,
  keywords      = "read next;unread;ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "2112.04035"
}

@ARTICLE{Zagha2022-fc,
  title    = "The Importance of Accounting for Movement When Relating Neuronal
              Activity to Sensory and Cognitive Processes",
  author   = "Zagha, Edward and Erlich, Jeffrey C and Lee, Soohyun and Lur,
              Gyorgy and O'Connor, Daniel H and Steinmetz, Nicholas A and
              Stringer, Carsen and Yang, Hongdian",
  abstract = "A surprising finding of recent studies in mouse is the dominance
              of widespread movement-related activity throughout the brain,
              including in early sensory areas. In awake subjects, failing to
              account for movement risks misattributing movement-related
              activity to other (e.g., sensory or cognitive) processes. In this
              article, we (1) review task designs for separating task-related
              and movement-related activity, (2) review three ``case studies''
              in which not considering movement would have resulted in
              critically different interpretations of neuronal function, and
              (3) discuss functional couplings that may prevent us from ever
              fully isolating sensory, motor, and cognitive-related activity.
              Our main thesis is that neural signals related to movement are
              ubiquitous, and therefore ought to be considered first and
              foremost when attempting to correlate neuronal activity with
              task-related processes.",
  journal  = "J. Neurosci.",
  volume   =  42,
  number   =  8,
  pages    = "1375--1382",
  month    =  feb,
  year     =  2022,
  keywords = "behavior; cognition; movement; neural coding; sensorimotor;4
              mouse;read next;unread;ERC Consolidator 2023",
  language = "en"
}

@UNPUBLISHED{Nayebi2021-mn,
  title    = "Shallow Unsupervised Models Best Predict Neural Responses in
              Mouse Visual Cortex",
  author   = "Nayebi, Aran and Kong, Nathan C L and Zhuang, Chengxu and
              Gardner, Justin L and Norcia, Anthony M and Yamins, Daniel L K",
  abstract = "Task-optimized deep convolutional neural networks are the most
              quantitatively accurate models of the primate ventral visual
              stream. However, such networks are implausible as models of the
              mouse visual system because mouse visual cortex has both lower
              retinal resolution and a shallower hierarchy than the primate.
              Moreover, the category supervision deep networks typically
              receive is neither ethologically relevant to the mouse in
              semantic content, nor realistic in quantity. As a result,
              standard supervised deep neural networks have proven
              quantitatively ineffective at modeling mouse visual data. Here,
              we develop and evaluate models that remedy these structural and
              functional gaps. We first demonstrate that shallow hierarchical
              architectures applied to lower resolution images improve match to
              neural responses, both in electro-physiological and calcium
              imaging data. We then show that networks trained using
              contrastive embedding methods, a recent unsupervised learning
              objective that requires no semantic labeling, achieve neural
              prediction performance that substantially exceed that of the same
              architectures trained in a supervised manner, across a wide
              variety of architecture types. Combining these better structural
              and functional priors yields models that are the most
              quantitatively accurate match to mouse visual responses to
              natural scenes, significantly surpassing that of prior attempts
              using primate-specific models, and approaching the inter-animal
              consistency level of the data itself. We further find that these
              shallow unsupervised models transfer to a wide variety of
              non-categorical visual tasks better than categorization-trained
              models. Taken together, our results suggest that mouse visual
              cortex is a low-resolution, shallow network that makes best use
              of the mouse's limited resources to create a light-weight,
              general-purpose visual system -- in contrast to the deep,
              high-resolution, and more task-specific visual system of
              primates. \#\#\# Competing Interest Statement The authors have
              declared no competing interest.",
  journal  = "bioRxiv",
  pages    = "2021.06.16.448730",
  month    =  aug,
  year     =  2021,
  keywords = "4 mouse;read next;unread;ERC Consolidator 2023",
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Patchell1971-zk,
  title     = "Separability, neutrality and certainty equivalence†",
  author    = "Patchell, J W and Jacobs, O L R",
  abstract  = "Abstract Studies of optimal stochastic control problems have
               drawn attention to three properties: separability, neutrality
               and certainty equivalence. The relationships between these
               properties have not yet been fully explored. This paper gives
               definitions of all three properties, summarizes well-known
               results about them and discusses the relationships between them.
               It is shown that separability is not the same as, but is a
               necessary condition for, certainty equivalence. It is
               conjectured that neutrality is a sufficient condition for
               certainty equivalence.",
  journal   = "Int. J. Control",
  publisher = "Taylor \& Francis",
  volume    =  13,
  number    =  2,
  pages     = "337--342",
  month     =  feb,
  year      =  1971,
  keywords  = "4 behavior;read next;unread;ERC Consolidator 2023"
}

@ARTICLE{Sajjadi2022-qb,
  title         = "Object Scene Representation Transformer",
  author        = "Sajjadi, Mehdi S M and Duckworth, Daniel and Mahendran,
                   Aravindh and van Steenkiste, Sjoerd and Paveti{\'c}, Filip
                   and Lu{\v c}i{\'c}, Mario and Guibas, Leonidas J and Greff,
                   Klaus and Kipf, Thomas",
  abstract      = "A compositional understanding of the world in terms of
                   objects and their geometry in 3D space is considered a
                   cornerstone of human cognition. Facilitating the learning of
                   such a representation in neural networks holds promise for
                   substantially improving labeled data efficiency. As a key
                   step in this direction, we make progress on the problem of
                   learning 3D-consistent decompositions of complex scenes into
                   individual objects in an unsupervised fashion. We introduce
                   Object Scene Representation Transformer (OSRT), a 3D-centric
                   model in which individual object representations naturally
                   emerge through novel view synthesis. OSRT scales to
                   significantly more complex scenes with larger diversity of
                   objects and backgrounds than existing methods. At the same
                   time, it is multiple orders of magnitude faster at
                   compositional rendering thanks to its light field
                   parametrization and the novel Slot Mixer decoder. We believe
                   this work will not only accelerate future architecture
                   exploration and scaling efforts, but it will also serve as a
                   useful tool for both object-centric as well as neural scene
                   representation learning communities.",
  month         =  jun,
  year          =  2022,
  keywords      = "ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2206.06922"
}

@ARTICLE{Schneider2022-qf,
  title         = "Learnable latent embeddings for joint behavioral and neural
                   analysis",
  author        = "Schneider, Steffen and Lee, Jin Hwa and Mathis, Mackenzie
                   Weygandt",
  abstract      = "Mapping behavioral actions to neural activity is a
                   fundamental goal of neuroscience. As our ability to record
                   large neural and behavioral data increases, there is growing
                   interest in modeling neural dynamics during adaptive
                   behaviors to probe neural representations. In particular,
                   neural latent embeddings can reveal underlying correlates of
                   behavior, yet, we lack non-linear techniques that can
                   explicitly and flexibly leverage joint behavior and neural
                   data. Here, we fill this gap with a novel method, CEBRA,
                   that jointly uses behavioral and neural data in a
                   hypothesis- or discovery-driven manner to produce
                   consistent, high-performance latent spaces. We validate its
                   accuracy and demonstrate our tool's utility for both calcium
                   and electrophysiology datasets, across sensory and motor
                   tasks, and in simple or complex behaviors across species. It
                   allows for single and multi-session datasets to be leveraged
                   for hypothesis testing or can be used label-free. Lastly, we
                   show that CEBRA can be used for the mapping of space,
                   uncovering complex kinematic features, and rapid,
                   high-accuracy decoding of natural movies from visual cortex.",
  month         =  apr,
  year          =  2022,
  keywords      = "skimmed;U19;ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2204.00673"
}

@UNPUBLISHED{Bowers2022-oq,
  title    = "Deep Problems with Neural Network Models of Human Vision",
  author   = "Bowers, Jeffrey S and Malhotra, Gaurav and Dujmovi{\'c}, Marin
              and Montero, Milton L and Tsvetkov, Christian and Biscione,
              Valerio and Puebla, Guillermo and Adolfi, Federico G and Hummel,
              John and Heaton, Rachel F and al., Et",
  abstract = "Deep neural networks (DNNs) have had extraordinary successes in
              classifying photographic images of objects and are often
              described as the best models of biological vision. This
              conclusion is largely based on three sets of findings: (1) DNNs
              are more accurate than any other model in classifying images
              taken from various datasets, (2) DNNs do the best job in
              predicting the pattern of human errors in classifying objects
              taken from various behavioral benchmark datasets, and (3) DNNs do
              the best job in predicting brain signals in response to images
              taken from various brain benchmark datasets (e.g., single cell
              responses or fMRI data). However, most behavioral and brain
              benchmarks report the outcomes of observational experiments that
              do not manipulate any independent variables, and we show that the
              good prediction on these datasets may be mediated by DNNs that
              share little overlap with biological vision. More
              problematically, we show that DNNs account for almost no results
              from psychological research. This contradicts the common claim
              that DNNs are good, let alone the best, models of human object
              recognition. We argue that theorists interested in developing
              biologically plausible models of human vision need to direct
              their attention to explaining psychological findings. More
              generally, theorists need to build models that explain the
              results of experiments that manipulate independent variables
              designed to test hypotheses rather than compete on predicting
              observational data. We conclude by briefly summarizing various
              promising modelling approaches that focus on psychological data.",
  month    =  apr,
  year     =  2022,
  keywords = "Brain-Score; Computational Neuroscience; Convolutional Neural
              Network; Deep Neural Networks; Human Vision; Object
              Identification; Object Recognition; Representational Similarity
              Analysis;unread;complexity;complexity;ERC Consolidator 2023"
}

@UNPUBLISHED{Di_Santo2022-zw,
  title    = "Unifying model for three forms of contextual modulation including
              feedback input from higher visual areas",
  author   = "Di Santo, Serena and Dipoppa, Mario and Keller, Andreas and Roth,
              Morgane and Scanziani, Massimo and Miller, Kenneth D",
  abstract = "Neural responses to a localized visual stimulus are modulated by
              the content of its surrounding. This phenomenon manifests in
              several forms of contextual modulation, including three
              interrelated properties of the visual cortex: surround
              suppression, inverse response and surround facilitation. We
              devise a unified biologically realistic circuit model accounting
              for all these phenomena and show that i) surround suppression in
              L2/3 is only partially due to the recruitment of lateral
              inhibition; ii) long-range feedback projections are necessary for
              inverse response and iii) the width of the response profile in
              the feedback layer determines inverse size tuning. The model
              predicts the modulations induced by silencing
              somatostatin-expressing cells or higher visual areas or changing
              the stimulus contrast. These predictions are consistent with the
              experimental observations when available and can be tested in
              existing setups otherwise. We then show the robustness of the
              identified mechanisms in a model with three interneuron
              subclasses, built to fit the classical responses and able to
              predict inverse size-tuning curves. \#\#\# Competing Interest
              Statement The authors have declared no competing interest.",
  journal  = "bioRxiv",
  pages    = "2022.05.27.493753",
  month    =  may,
  year     =  2022,
  keywords = "4 mouse;read next;unread;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Kipf2021-zk,
  title         = "Conditional {Object-Centric} Learning from Video",
  author        = "Kipf, Thomas and Elsayed, Gamaleldin F and Mahendran,
                   Aravindh and Stone, Austin and Sabour, Sara and Heigold,
                   Georg and Jonschkowski, Rico and Dosovitskiy, Alexey and
                   Greff, Klaus",
  abstract      = "Object-centric representations are a promising path toward
                   more systematic generalization by providing flexible
                   abstractions upon which compositional world models can be
                   built. Recent work on simple 2D and 3D datasets has shown
                   that models with object-centric inductive biases can learn
                   to segment and represent meaningful objects from the
                   statistical structure of the data alone without the need for
                   any supervision. However, such fully-unsupervised methods
                   still fail to scale to diverse realistic data, despite the
                   use of increasingly complex inductive biases such as priors
                   for the size of objects or the 3D geometry of the scene. In
                   this paper, we instead take a weakly-supervised approach and
                   focus on how 1) using the temporal dynamics of video data in
                   the form of optical flow and 2) conditioning the model on
                   simple object location cues can be used to enable segmenting
                   and tracking objects in significantly more realistic
                   synthetic data. We introduce a sequential extension to Slot
                   Attention which we train to predict optical flow for
                   realistic looking synthetic scenes and show that
                   conditioning the initial state of this model on a small set
                   of hints, such as center of mass of objects in the first
                   frame, is sufficient to significantly improve instance
                   segmentation. These benefits generalize beyond the training
                   distribution to novel objects, novel backgrounds, and to
                   longer video sequences. We also find that such
                   initial-state-conditioning can be used during inference as a
                   flexible interface to query the model for specific objects
                   or parts of objects, which could pave the way for a range of
                   weakly-supervised approaches and allow more effective
                   interaction with trained models.",
  month         =  nov,
  year          =  2021,
  keywords      = "skimmed;ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2111.12594"
}

@ARTICLE{Wang2022-cn,
  title    = "Tuning landscapes of the ventral stream",
  author   = "Wang, Binxu and Ponce, Carlos R",
  abstract = "A goal in visual neuroscience is to explain how neurons respond
              to natural scenes. However, neurons are generally tested using
              simpler stimuli, often because they can be transformed smoothly,
              allowing the measurement of tuning functions (i.e., response
              peaks and slopes). Here, we test the idea that all classic tuning
              curves can be viewed as slices of a higher-dimensional tuning
              landscape. We use activation-maximizing stimuli (``prototypes'')
              as landmarks in a generative image space and map tuning functions
              around these peaks. We find that neurons show smooth bell-shaped
              tuning consistent with radial basis functions, spanning a vast
              image transformation range, with systematic differences in
              landscape geometry from V1 to inferotemporal cortex. By modeling
              these trends, we infer that neurons in the higher visual cortex
              have higher intrinsic feature dimensionality. Overall, these
              results suggest that visual neurons are better viewed as
              signaling distances to prototypes on an image manifold.",
  journal  = "Cell Rep.",
  volume   =  41,
  number   =  6,
  pages    = "111595",
  month    =  nov,
  year     =  2022,
  keywords = "CP: Neuroscience; V1; V4; generative image model; geometry;
              inferotemporal cortex; intrinsic dimensionality; natural image
              manifold; neural tuning; neuron-guided image synthesis; radial
              basis function; tuning; vision; visual cortex; visual
              hierarchy;unread;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Glickfeld2017-fk,
  title    = "{Higher-Order} Areas of the Mouse Visual Cortex",
  author   = "Glickfeld, Lindsey L and Olsen, Shawn R",
  abstract = "The brain has evolved to transform sensory information in the
              environment into neural representations that can be used for
              perception and action. Higher-order sensory cortical areas, with
              their increasingly complex receptive fields and integrative
              properties, are thought to be critical nodes for this function.
              This is especially true in the primate visual cortex, in which
              functionally specialized areas are engaged in parallel streams to
              support diverse computations. Recent anatomical and physiological
              studies of the mouse visual cortex have revealed a similarly
              complex network of specialized higher-order areas. This structure
              provides a useful model for determining the synaptic and circuit
              mechanisms through which information is transformed across
              distinct processing stages. In this review, we summarize the
              current knowledge on the layout, connectivity, and functional
              properties of the higher visual areas in the mouse. In addition,
              we speculate on the contribution of these areas to perception and
              action, and how knowledge of the mouse visual system can inform
              us about the principles that govern information processing in
              integrated networks.",
  journal  = "Annu Rev Vis Sci",
  volume   =  3,
  pages    = "251--273",
  month    =  sep,
  year     =  2017,
  keywords = "connectivity; functional specialization; hierarchical and
              parallel processing; higher visual area; mouse; visual cortex;4
              mouse;read next;unread;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Kalweit2020-ru,
  title    = "Deep inverse q-learning with constraints",
  author   = "Kalweit, Gabriel and Huegle, Maria and Werling, Moritz and
              Boedecker, Joschka",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   =  33,
  pages    = "14291--14302",
  year     =  2020,
  keywords = "skimmed;ERC Consolidator 2023"
}

@ARTICLE{Conwell2021-pw,
  title    = "Neural regression, representational similarity, model zoology \&
              neural taskonomy at scale in rodent visual cortex",
  author   = "Conwell, Colin and Mayo, David and Barbu, Andrei and Buice,
              Michael and Alvarez, George and Katz, Boris",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   =  34,
  pages    = "5590--5607",
  year     =  2021,
  keywords = "read;4 mouse;ERC Consolidator 2023"
}

@ARTICLE{Urai2022-fz,
  title     = "Large-scale neural recordings call for new insights to link
               brain and behavior",
  author    = "Urai, Anne E and Doiron, Brent and Leifer, Andrew M and
               Churchland, Anne K",
  abstract  = "Neuroscientists today can measure activity from more neurons
               than ever before, and are facing the challenge of connecting
               these brain-wide neural recordings to computation and behavior.
               In the present review, we first describe emerging tools and
               technologies being used to probe large-scale brain activity and
               new approaches to characterize behavior in the context of such
               measurements. We next highlight insights obtained from
               large-scale neural recordings in diverse model systems, and
               argue that some of these pose a challenge to traditional
               theoretical frameworks. Finally, we elaborate on existing
               modeling frameworks to interpret these data, and argue that the
               interpretation of brain-wide neural recordings calls for new
               theoretical approaches that may depend on the desired level of
               understanding. These advances in both neural recordings and
               theory development will pave the way for critical advances in
               our understanding of the brain. Neuroscientists can measure
               activity from more neurons than ever before, garnering new
               insights and posing challenges to traditional theoretical
               frameworks. New frameworks may help researchers use these
               observations to shed light on brain function.",
  journal   = "Nat. Neurosci.",
  publisher = "Nature Publishing Group",
  volume    =  25,
  number    =  1,
  pages     = "11--19",
  month     =  jan,
  year      =  2022,
  keywords  = "4 behavior;4 mouse;read;ERC Consolidator 2023",
  language  = "en"
}

@ARTICLE{Juavinett2018-oz,
  title    = "Decision-making behaviors: weighing ethology, complexity, and
              sensorimotor compatibility",
  author   = "Juavinett, Ashley L and Erlich, Jeffrey C and Churchland, Anne K",
  abstract = "Rodent decision-making research aims to uncover the neural
              circuitry underlying the ability to evaluate alternatives and
              select appropriate actions. Designing behavioral paradigms that
              provide a solid foundation to ask questions about decision-making
              computations and mechanisms is a difficult and often
              underestimated challenge. Here, we propose three dimensions on
              which we can consider rodent decision-making tasks: ethological
              validity, task complexity, and stimulus-response compatibility.
              We review recent research through this lens, and provide
              practical guidance for researchers in the decision-making field.",
  journal  = "Curr. Opin. Neurobiol.",
  volume   =  49,
  pages    = "42--50",
  month    =  apr,
  year     =  2018,
  keywords = "4 behavior;4 mouse;read next;unread;ERC Consolidator 2023"
}

@ARTICLE{Marshel2019-ph,
  title    = "Cortical layer-specific critical dynamics triggering perception",
  author   = "Marshel, James H and Kim, Yoon Seok and Machado, Timothy A and
              Quirin, Sean and Benson, Brandon and Kadmon, Jonathan and Raja,
              Cephra and Chibukhchyan, Adelaida and Ramakrishnan, Charu and
              Inoue, Masatoshi and Shane, Janelle C and McKnight, Douglas J and
              Yoshizawa, Susumu and Kato, Hideaki E and Ganguli, Surya and
              Deisseroth, Karl",
  abstract = "Perceptual experiences may arise from neuronal activity patterns
              in mammalian neocortex. We probed mouse neocortex during visual
              discrimination using a red-shifted channelrhodopsin (ChRmine,
              discovered through structure-guided genome mining) alongside
              multiplexed multiphoton-holography (MultiSLM), achieving control
              of individually specified neurons spanning large cortical volumes
              with millisecond precision. Stimulating a critical number of
              stimulus-orientation-selective neurons drove widespread
              recruitment of functionally related neurons, a process enhanced
              by (but not requiring) orientation-discrimination task learning.
              Optogenetic targeting of orientation-selective ensembles elicited
              correct behavioral discrimination. Cortical layer-specific
              dynamics were apparent, as emergent neuronal activity
              asymmetrically propagated from layer 2/3 to layer 5, and smaller
              layer 5 ensembles were as effective as larger layer 2/3 ensembles
              in eliciting orientation discrimination behavior. Population
              dynamics emerging after optogenetic stimulation both correctly
              predicted behavior and resembled natural internal representations
              of visual stimuli at cellular resolution over volumes of cortex.",
  journal  = "Science",
  volume   =  365,
  number   =  6453,
  month    =  aug,
  year     =  2019,
  keywords = "unread;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Franke2022-do,
  title    = "State-dependent pupil dilation rapidly shifts visual feature
              selectivity",
  author   = "Franke, Katrin and Willeke, Konstantin F and Ponder, Kayla and
              Galdamez, Mario and Zhou, Na and Muhammad, Taliah and Patel,
              Saumil and Froudarakis, Emmanouil and Reimer, Jacob and Sinz,
              Fabian H and Tolias, Andreas S",
  abstract = "To increase computational flexibility, the processing of sensory
              inputs changes with behavioural context. In the visual system,
              active behavioural states characterized by motor activity and
              pupil dilation1,2 enhance sensory responses, but typically leave
              the preferred stimuli of neurons unchanged2-9. Here we find that
              behavioural state also modulates stimulus selectivity in the
              mouse visual cortex in the context of coloured natural scenes.
              Using population imaging in behaving mice, pharmacology and deep
              neural network modelling, we identified a rapid shift in colour
              selectivity towards ultraviolet stimuli during an active
              behavioural state. This was exclusively caused by state-dependent
              pupil dilation, which resulted in a dynamic switch from rod to
              cone photoreceptors, thereby extending their role beyond night
              and day vision. The change in tuning facilitated the decoding of
              ethological stimuli, such as aerial predators against the
              twilight sky10. For decades, studies in neuroscience and
              cognitive science have used pupil dilation as an indirect measure
              of brain state. Our data suggest that, in addition,
              state-dependent pupil dilation itself tunes visual
              representations to behavioural demands by differentially
              recruiting rods and cones on fast timescales.",
  journal  = "Nature",
  volume   =  610,
  number   =  7930,
  pages    = "128--134",
  month    =  oct,
  year     =  2022,
  keywords = "read;ERC Consolidator 2023",
  language = "en"
}

@INPROCEEDINGS{Lurz2022-up,
  title     = "Bayesian Oracle for bounding information gain in neural encoding
               models",
  booktitle = "Neurips 2022 Workshop {InfoCog}",
  author    = "Lurz, Konstantin and Bashiri, Mohammad and Sinz, Fabian",
  abstract  = "In recent years, deep learning models have set new standards in
               predicting neural population responses. Most of these models
               currently focus on predicting the mean response of each neuron
               for a given input. However, neural variability around this mean
               is not just noise and plays a central role in several theories
               on neural computation. To capture this variability, we need
               models that predict full response distributions for a given
               stimulus. However, to measure the quality of such models,
               commonly used correlation-based metrics are not sufficient as
               they mainly care about the mean of the response distribution. An
               interpretable alternative evaluation metric for likelihood-based
               models is \textbackslashtextit\{Information Gain\} (IG) which
               evaluates the likelihood of a model relative to a lower and
               upper bound. However, while a lower bound is usually easy to
               obtain, constructing an upper bound turns out to be challenging
               for neural recordings with relatively low numbers of repeated
               trials, high (shared) variability, and sparse responses. In this
               work, we generalize the jack-knife oracle estimator for the
               mean---commonly used for correlation metrics---to a flexible
               Bayesian oracle estimator for IG based on posterior predictive
               distributions. We describe and address the challenges that arise
               when estimating the lower and upper bounds from small datasets.
               We then show that our upper bound estimate is data-efficient and
               robust even in the case of sparse responses and low
               signal-to-noise ratio. We further provide the derivation of the
               upper bound estimator for a variety of common distributions
               including the state-of-the-art zero-inflated mixture models, and
               relate IG to common mean-based metrics. Finally, we use our
               approach to evaluate such a mixture model resulting in $90\%$ IG
               performance.",
  month     =  nov,
  year      =  2022,
  keywords  = "read;own;ERC Consolidator 2023"
}

@ARTICLE{Willeke2022-qu,
  title         = "The Sensorium competition on predicting large-scale mouse
                   primary visual cortex activity",
  author        = "Willeke, Konstantin F and Fahey, Paul G and Bashiri,
                   Mohammad and Pede, Laura and Burg, Max F and Blessing,
                   Christoph and Cadena, Santiago A and Ding, Zhiwei and Lurz,
                   Konstantin-Klemens and Ponder, Kayla and Muhammad, Taliah
                   and Patel, Saumil S and Ecker, Alexander S and Tolias,
                   Andreas S and Sinz, Fabian H",
  abstract      = "The neural underpinning of the biological visual system is
                   challenging to study experimentally, in particular as the
                   neuronal activity becomes increasingly nonlinear with
                   respect to visual input. Artificial neural networks (ANNs)
                   can serve a variety of goals for improving our understanding
                   of this complex system, not only serving as predictive
                   digital twins of sensory cortex for novel hypothesis
                   generation in silico, but also incorporating bio-inspired
                   architectural motifs to progressively bridge the gap between
                   biological and machine vision. The mouse has recently
                   emerged as a popular model system to study visual
                   information processing, but no standardized large-scale
                   benchmark to identify state-of-the-art models of the mouse
                   visual system has been established. To fill this gap, we
                   propose the Sensorium benchmark competition. We collected a
                   large-scale dataset from mouse primary visual cortex
                   containing the responses of more than 28,000 neurons across
                   seven mice stimulated with thousands of natural images,
                   together with simultaneous behavioral measurements that
                   include running speed, pupil dilation, and eye movements.
                   The benchmark challenge will rank models based on predictive
                   performance for neuronal responses on a held-out test set,
                   and includes two tracks for model input limited to either
                   stimulus only (Sensorium) or stimulus plus behavior
                   (Sensorium+). We provide a starting kit to lower the barrier
                   for entry, including tutorials, pre-trained baseline models,
                   and APIs with one line commands for data loading and
                   submission. We would like to see this as a starting point
                   for regular challenges and data releases, and as a standard
                   tool for measuring progress in large-scale neural system
                   identification models of the mouse visual system and beyond.",
  month         =  jun,
  year          =  2022,
  keywords      = "read;own;ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2206.08666"
}

@UNPUBLISHED{Cadena2022-im,
  title    = "Diverse task-driven modeling of macaque {V4} reveals functional
              specialization towards semantic tasks",
  author   = "Cadena, Santiago A and Willeke, Konstantin F and Restivo, Kelli
              and Denfield, George and Sinz, Fabian H and Bethge, Matthias and
              Tolias, Andreas S and Ecker, Alexander S",
  abstract = "Responses to natural stimuli in area V4 -- a mid-level area of
              the visual ventral stream -- are well predicted by features from
              convolutional neural networks (CNNs) trained on image
              classification. This result has been taken as evidence for the
              functional role of V4 in object classification. However, we
              currently do not know if and to what extent V4 plays a role in
              solving other computational objectives. Here, we investigated
              normative accounts of V4 by predicting macaque single-neuron
              responses to natural images from the representations extracted by
              23 CNNs trained on different computer vision tasks including
              semantic, geometric, 2D, and 3D visual tasks. We found that
              semantic classification tasks do indeed provide the best
              predictive features for V4. Other tasks (3D in particular)
              followed very closely in performance, but a similar pattern of
              tasks performance emerged when predicting the activations of a
              network exclusively trained on object recognition. Thus, our
              results support V4's main functional role in semantic processing.
              At the same time, they suggest that V4's affinity to various 3D
              and 2D stimulus features found by electrophysiologists could be a
              corollary of a semantic functional goal. \#\#\# Competing
              Interest Statement The authors have declared no competing
              interest.",
  journal  = "bioRxiv",
  pages    = "2022.05.18.492503",
  month    =  may,
  year     =  2022,
  keywords = "read;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Schultheis2021-pu,
  title    = "Inverse optimal control adapted to the noise characteristics of
              the human sensorimotor system",
  author   = "Schultheis, Matthias and Straub, Dominik and Rothkopf, Constantin
              A",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   =  34,
  pages    = "9429--9442",
  year     =  2021,
  keywords = "skimmed;ERC Consolidator 2023"
}

@ARTICLE{Straub2022-we,
  title    = "Putting perception into action with inverse optimal control for
              continuous psychophysics",
  author   = "Straub, Dominik and Rothkopf, Constantin A",
  abstract = "Psychophysical methods are a cornerstone of psychology, cognitive
              science, and neuroscience where they have been used to quantify
              behavior and its neural correlates for a vast range of mental
              phenomena. Their power derives from the combination of controlled
              experiments and rigorous analysis through signal detection
              theory. Unfortunately, they require many tedious trials and
              preferably highly trained participants. A recently developed
              approach, continuous psychophysics, promises to transform the
              field by abandoning the rigid trial structure involving binary
              responses and replacing it with continuous behavioral adjustments
              to dynamic stimuli. However, what has precluded wide adoption of
              this approach is that current analysis methods do not account for
              the additional variability introduced by the motor component of
              the task and therefore recover perceptual thresholds that are
              larger compared to equivalent traditional psychophysical
              experiments. Here, we introduce a computational analysis
              framework for continuous psychophysics based on Bayesian inverse
              optimal control. We show via simulations and previously published
              data that this not only recovers the perceptual thresholds but
              additionally estimates subjects' action variability, internal
              behavioral costs, and subjective beliefs about the experimental
              stimulus dynamics. Taken together, we provide further evidence
              for the importance of including acting uncertainties, subjective
              beliefs, and, crucially, the intrinsic costs of behavior, even in
              experiments seemingly only investigating perception.",
  journal  = "Elife",
  volume   =  11,
  month    =  sep,
  year     =  2022,
  keywords = "continuous psychophysics; human; inverse reinforcement learning;
              neuroscience; optimal control; perception and action; rational
              analysis;read;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Watters2019-qy,
  title         = "Spatial Broadcast Decoder: A Simple Architecture for
                   Learning Disentangled Representations in {VAEs}",
  author        = "Watters, Nicholas and Matthey, Loic and Burgess, Christopher
                   P and Lerchner, Alexander",
  abstract      = "We present a simple neural rendering architecture that helps
                   variational autoencoders (VAEs) learn disentangled
                   representations. Instead of the deconvolutional network
                   typically used in the decoder of VAEs, we tile (broadcast)
                   the latent vector across space, concatenate fixed X- and
                   Y-``coordinate'' channels, and apply a fully convolutional
                   network with 1x1 stride. This provides an architectural
                   prior for dissociating positional from non-positional
                   features in the latent distribution of VAEs, yet without
                   providing any explicit supervision to this effect. We show
                   that this architecture, which we term the Spatial Broadcast
                   decoder, improves disentangling, reconstruction accuracy,
                   and generalization to held-out regions in data space. It
                   provides a particularly dramatic benefit when applied to
                   datasets with small objects. We also emphasize a method for
                   visualizing learned latent spaces that helped us diagnose
                   our models and may prove useful for others aiming to assess
                   data representations. Finally, we show the Spatial Broadcast
                   Decoder is complementary to state-of-the-art (SOTA)
                   disentangling techniques and when incorporated improves
                   their performance.",
  month         =  jan,
  year          =  2019,
  keywords      = "read next;unread;ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1901.07017"
}

@ARTICLE{Locatello2020-io,
  title         = "Object-centric learning with Slot Attention",
  author        = "Locatello, Francesco and Weissenborn, Dirk and Unterthiner,
                   Thomas and Mahendran, Aravindh and Heigold, Georg and
                   Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas",
  abstract      = "Learning object-centric representations of complex scenes is
                   a promising step towards enabling efficient abstract
                   reasoning from low-level perceptual features. Yet, most deep
                   learning approaches learn distributed representations that
                   do not capture the compositional properties of natural
                   scenes. In this paper, we present the Slot Attention module,
                   an architectural component that interfaces with perceptual
                   representations such as the output of a convolutional neural
                   network and produces a set of task-dependent abstract
                   representations which we call slots. These slots are
                   exchangeable and can bind to any object in the input by
                   specializing through a competitive procedure over multiple
                   rounds of attention. We empirically demonstrate that Slot
                   Attention can extract object-centric representations that
                   enable generalization to unseen compositions when trained on
                   unsupervised object discovery and supervised property
                   prediction tasks.",
  month         =  jun,
  year          =  2020,
  keywords      = "read;ERC Consolidator 2023",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2006.15055"
}

@INPROCEEDINGS{Hoogeboom2021-zs,
  title     = "Learning Discrete Distributions by Dequantization",
  booktitle = "Third Symposium on Advances in Approximate Bayesian Inference",
  author    = "Hoogeboom, Emiel and Cohen, Taco and Tomczak, Jakub Mikolaj",
  year      =  2021,
  keywords  = "read;ERC Consolidator 2023"
}

@INPROCEEDINGS{Bashiri2021-or,
  title     = "A flow-based latent state generative model of neural population
               responses to natural images",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Bashiri, Mohammad and Walker, Edgar and Lurz, Konstantin-Klemens
               and Jagadish, Akshay and Muhammad, Taliah and Ding, Zhiwei and
               Ding, Zhuokun and Tolias, Andreas and Sinz, Fabian",
  editor    = "Ranzato, M and Beygelzimer, A and Dauphin, Y and Liang, P S and
               Vaughan, J Wortman",
  publisher = "Curran Associates, Inc.",
  volume    =  34,
  pages     = "15801--15815",
  month     =  dec,
  year      =  2021,
  keywords  = "own;SFB 1456 Mathematic of
               Experiment;NeurIPS-2021-a-flow-based-latent-state-generative-model-of-neural-population-responses-to-natural-images-Bibtex.bib;ERC
               Consolidator 2023"
}

@ARTICLE{Kalweit2022-ev,
  title         = "{NeuRL}: Closed-form Inverse Reinforcement Learning for
                   Neural Decoding",
  author        = "Kalweit, Gabriel and Kalweit, Maria and Alyahyay, Mansour
                   and Jaeckel, Zoe and Steenbergen, Florian and Hardung,
                   Stefanie and Brox, Thomas and Diester, Ilka and Boedecker,
                   Joschka",
  abstract      = "Current neural decoding methods typically aim at explaining
                   behavior based on neural activity via supervised learning.
                   However, since generally there is a strong connection
                   between learning of subjects and their expectations on
                   long-term rewards, we propose NeuRL, an inverse
                   reinforcement learning approach that (1) extracts an
                   intrinsic reward function from collected trajectories of a
                   subject in closed form, (2) maps neural signals to this
                   intrinsic reward to account for long-term dependencies in
                   the behavior and (3) predicts the simulated behavior for
                   unseen neural signals by extracting Q-values and the
                   corresponding Boltzmann policy based on the intrinsic reward
                   values for these unseen neural signals. We show that NeuRL
                   leads to better generalization and improved decoding
                   performance compared to supervised approaches. We study the
                   behavior of rats in a response-preparation task and evaluate
                   the performance of NeuRL within simulated inhibition and
                   per-trial behavior prediction. By assigning clear functional
                   roles to defined neuronal populations our approach offers a
                   new interpretation tool for complex neuronal data with
                   testable predictions. In per-trial behavior prediction, our
                   approach furthermore improves accuracy by up to 15\%
                   compared to traditional methods.",
  month         =  apr,
  year          =  2022,
  keywords      = "4 mouse;read;ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2204.04733"
}

@ARTICLE{Arora2018-tq,
  title         = "A Survey of Inverse Reinforcement Learning: Challenges,
                   Methods and Progress",
  author        = "Arora, Saurabh and Doshi, Prashant",
  abstract      = "Inverse reinforcement learning (IRL) is the problem of
                   inferring the reward function of an agent, given its policy
                   or observed behavior. Analogous to RL, IRL is perceived both
                   as a problem and as a class of methods. By categorically
                   surveying the current literature in IRL, this article serves
                   as a reference for researchers and practitioners of machine
                   learning and beyond to understand the challenges of IRL and
                   select the approaches best suited for the problem on hand.
                   The survey formally introduces the IRL problem along with
                   its central challenges such as the difficulty in performing
                   accurate inference and its generalizability, its sensitivity
                   to prior knowledge, and the disproportionate growth in
                   solution complexity with problem size. The article
                   elaborates how the current methods mitigate these
                   challenges. We further discuss the extensions to traditional
                   IRL methods for handling: inaccurate and incomplete
                   perception, an incomplete model, multiple reward functions,
                   and nonlinear reward functions. This survey concludes the
                   discussion with some broad advances in the research area and
                   currently open research questions.",
  month         =  jun,
  year          =  2018,
  keywords      = "skimmed;ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1806.06877"
}

@ARTICLE{Merel2019-qm,
  title         = "Deep neuroethology of a virtual rodent",
  author        = "Merel, Josh and Aldarondo, Diego and Marshall, Jesse and
                   Tassa, Yuval and Wayne, Greg and {\"O}lveczky, Bence",
  abstract      = "Parallel developments in neuroscience and deep learning have
                   led to mutually productive exchanges, pushing our
                   understanding of real and artificial neural networks in
                   sensory and cognitive systems. However, this interaction
                   between fields is less developed in the study of motor
                   control. In this work, we develop a virtual rodent as a
                   platform for the grounded study of motor activity in
                   artificial models of embodied control. We then use this
                   platform to study motor activity across contexts by training
                   a model to solve four complex tasks. Using methods familiar
                   to neuroscientists, we describe the behavioral
                   representations and algorithms employed by different layers
                   of the network using a neuroethological approach to
                   characterize motor activity relative to the rodent's
                   behavior and goals. We find that the model uses two classes
                   of representations which respectively encode the
                   task-specific behavioral strategies and task-invariant
                   behavioral kinematics. These representations are reflected
                   in the sequential activity and population dynamics of neural
                   subpopulations. Overall, the virtual rodent facilitates
                   grounded collaborations between deep reinforcement learning
                   and motor neuroscience.",
  month         =  nov,
  year          =  2019,
  keywords      = "4 mouse;read;ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "1911.09451"
}

@ARTICLE{Cohen2022-hy,
  title    = "Recent Advances at the Interface of Neuroscience and Artificial
              Neural Networks",
  author   = "Cohen, Yarden and Engel, Tatiana A and Langdon, Christopher and
              Lindsay, Grace W and Ott, Torben and Peters, Megan A K and Shine,
              James M and Breton-Provencher, Vincent and Ramaswamy, Srikanth",
  abstract = "Biological neural networks adapt and learn in diverse behavioral
              contexts. Artificial neural networks (ANNs) have exploited
              biological properties to solve complex problems. However, despite
              their effectiveness for specific tasks, ANNs are yet to realize
              the flexibility and adaptability of biological cognition. This
              review highlights recent advances in computational and
              experimental research to advance our understanding of biological
              and artificial intelligence. In particular, we discuss critical
              mechanisms from the cellular, systems, and cognitive neuroscience
              fields that have contributed to refining the architecture and
              training algorithms of ANNs. Additionally, we discuss how recent
              work used ANNs to understand complex neuronal correlates of
              cognition and to process high throughput behavioral data.",
  journal  = "J. Neurosci.",
  volume   =  42,
  number   =  45,
  pages    = "8514--8523",
  month    =  nov,
  year     =  2022,
  keywords = "artificial neural networks; behavior; cognition; neuromodulators;
              plasticity; vision;4 mouse;read next;unread;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Doerig2022-ex,
  title         = "The neuroconnectionist research programme",
  author        = "Doerig, Adrien and Sommers, Rowan and Seeliger, Katja and
                   Richards, Blake and Ismael, Jenann and Lindsay, Grace and
                   Kording, Konrad and Konkle, Talia and Van Gerven, Marcel A J
                   and Kriegeskorte, Nikolaus and Kietzmann, Tim C",
  abstract      = "Artificial Neural Networks (ANNs) inspired by biology are
                   beginning to be widely used to model behavioral and neural
                   data, an approach we call neuroconnectionism. ANNs have been
                   lauded as the current best models of information processing
                   in the brain, but also criticized for failing to account for
                   basic cognitive functions. We propose that arguing about the
                   successes and failures of a restricted set of current ANNs
                   is the wrong approach to assess the promise of
                   neuroconnectionism. Instead, we take inspiration from the
                   philosophy of science, and in particular from Lakatos, who
                   showed that the core of scientific research programmes is
                   often not directly falsifiable, but should be assessed by
                   its capacity to generate novel insights. Following this
                   view, we present neuroconnectionism as a cohesive
                   large-scale research programme centered around ANNs as a
                   computational language for expressing falsifiable theories
                   about brain computation. We describe the core of the
                   programme, the underlying computational framework and its
                   tools for testing specific neuroscientific hypotheses.
                   Taking a longitudinal view, we review past and present
                   neuroconnectionist projects and their responses to
                   challenges, and argue that the research programme is highly
                   progressive, generating new and otherwise unreachable
                   insights into the workings of the brain.",
  month         =  sep,
  year          =  2022,
  keywords      = "skimmed;ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2209.03718"
}

@ARTICLE{Barlow1972-eh,
  title    = "Single units and sensation: A neuron doctrine for perceptual
              psychology?",
  author   = "Barlow, H B",
  journal  = "Perception",
  volume   =  1,
  pages    = "371--394",
  year     =  1972,
  keywords = "read;read next;\_imported;ERC Consolidator 2023"
}

@UNPUBLISHED{Merel2022-hf,
  title    = "Deep neuroethology of a virtual rodent",
  author   = "Merel, Josh and Aldarondo, Diego and Marshall, Jesse and Tassa,
              Yuval and Wayne, Greg and Olveczky, Bence",
  abstract = "Parallel developments in neuroscience and deep learning have led
              to mutually productive exchanges, pushing our understanding of
              real and artificial neural networks in sensory and cognitive
              systems. However, this interaction between fields is less
              developed in the study of motor control. In this work, we develop
              a virtual rodent as a platform for the grounded study of motor
              activity in artificial models of embodied control. We then use
              this platform to study motor activity across contexts by training
              a model to solve four complex tasks. Using methods familiar to
              neuroscientists, we describe the behavioral representations and
              algorithms employed by different layers of the network using a
              neuroethological approach to characterize motor activity relative
              to the rodent's behavior and goals. We find that the model uses
              two classes of representations which respectively encode the
              task-specific behavioral strategies and task-invariant behavioral
              kinematics. These representations are reflected in the sequential
              activity and population dynamics of neural subpopulations.
              Overall, the virtual rodent facilitates grounded collaborations
              between deep reinforcement learning and motor neuroscience.",
  month    =  feb,
  year     =  2022,
  keywords = "skimmed;4 mouse;4 behavior;ERC Consolidator 2023"
}

@ARTICLE{Pierzchlewicz2022-tq,
  title         = "Multi-hypothesis {3D} human pose estimation metrics favor
                   miscalibrated distributions",
  author        = "Pierzchlewicz, Pawe{\l} A and James Cotton, R and Bashiri,
                   Mohammad and Sinz, Fabian H",
  abstract      = "Due to depth ambiguities and occlusions, lifting 2D poses to
                   3D is a highly ill-posed problem. Well-calibrated
                   distributions of possible poses can make these ambiguities
                   explicit and preserve the resulting uncertainty for
                   downstream tasks. This study shows that previous attempts,
                   which account for these ambiguities via multiple hypotheses
                   generation, produce miscalibrated distributions. We identify
                   that miscalibration can be attributed to the use of
                   sample-based metrics such as minMPJPE. In a series of
                   simulations, we show that minimizing minMPJPE, as commonly
                   done, should converge to the correct mean prediction.
                   However, it fails to correctly capture the uncertainty, thus
                   resulting in a miscalibrated distribution. To mitigate this
                   problem, we propose an accurate and well-calibrated model
                   called Conditional Graph Normalizing Flow (cGNFs). Our model
                   is structured such that a single cGNF can estimate both
                   conditional and marginal densities within the same model -
                   effectively solving a zero-shot density estimation problem.
                   We evaluate cGNF on the Human~3.6M dataset and show that
                   cGNF provides a well-calibrated distribution estimate while
                   being close to state-of-the-art in terms of overall
                   minMPJPE. Furthermore, cGNF outperforms previous methods on
                   occluded joints while it remains well-calibrated.",
  month         =  oct,
  year          =  2022,
  keywords      = "ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2210.11179"
}

@ARTICLE{Richards2019-lm,
  title    = "A deep learning framework for neuroscience",
  author   = "Richards, Blake A and Lillicrap, Timothy P and Beaudoin, Philippe
              and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and
              Clopath, Claudia and Costa, Rui Ponte and de Berker, Archy and
              Ganguli, Surya and Gillon, Colleen J and Hafner, Danijar and
              Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and
              Lindsay, Grace W and Miller, Kenneth D and Naud, Richard and
              Pack, Christopher C and Poirazi, Panayiota and Roelfsema, Pieter
              and Sacramento, Jo{\~a}o and Saxe, Andrew and Scellier, Benjamin
              and Schapiro, Anna C and Senn, Walter and Wayne, Greg and Yamins,
              Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien,
              Denis and Kording, Konrad P",
  abstract = "Systems neuroscience seeks explanations for how the brain
              implements a wide variety of perceptual, cognitive and motor
              tasks. Conversely, artificial intelligence attempts to design
              computational systems based on the tasks they will have to solve.
              In artificial neural networks, the three components specified by
              design are the objective functions, the learning rules and the
              architectures. With the growing success of deep learning, which
              utilizes brain-inspired architectures, these three designed
              components have increasingly become central to how we model,
              engineer and optimize complex artificial learning systems. Here
              we argue that a greater focus on these components would also
              benefit systems neuroscience. We give examples of how this
              optimization-based framework can drive theoretical and
              experimental progress in neuroscience. We contend that this
              principled perspective on systems neuroscience will help to
              generate more rapid progress.",
  journal  = "Nat. Neurosci.",
  volume   =  22,
  number   =  11,
  pages    = "1761--1770",
  month    =  nov,
  year     =  2019,
  keywords = "4 mouse;read;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Colonnier1981-oj,
  title    = "[Number of neurons and synapses in the visual cortex of different
              species]",
  author   = "Colonnier, M and O'Kusky, J",
  abstract = "The number of neurons under 1 mm2 of visual cortex (area 17) is
              about 200 000 in monkey and man, and it varies between 45 000 and
              70 000 in non-primates which have been studied. The number per
              hemisphere increases with the surface of area 17, passing from
              less than 1 million in mouse to about 538 million in man. The
              number of synapses under 1 mm2 of visual cortex has been
              estimated by different authors at between 480 million (mouse) and
              1270 million (rat) : the number per hemisphere increases with
              brain size from 32 billion in rat to 3 084 billion (x10(9)) in
              man. The number of synapses per neurons tends to be higher in
              species with fewer neurons per mm3. Our laminar study in monkey
              shows this correlation at the level of each lamina : those having
              the largest number of neurons per mm3 have the least number of
              synapses per neuron.",
  journal  = "Rev. Can. Biol.",
  volume   =  40,
  number   =  1,
  pages    = "91--99",
  month    =  mar,
  year     =  1981,
  keywords = "ERC Consolidator 2023",
  language = "fr"
}

@ARTICLE{Chen2021-ap,
  title         = "Decision Transformer: Reinforcement Learning via Sequence
                   Modeling",
  author        = "Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee,
                   Kimin and Grover, Aditya and Laskin, Michael and Abbeel,
                   Pieter and Srinivas, Aravind and Mordatch, Igor",
  abstract      = "We introduce a framework that abstracts Reinforcement
                   Learning (RL) as a sequence modeling problem. This allows us
                   to draw upon the simplicity and scalability of the
                   Transformer architecture, and associated advances in
                   language modeling such as GPT-x and BERT. In particular, we
                   present Decision Transformer, an architecture that casts the
                   problem of RL as conditional sequence modeling. Unlike prior
                   approaches to RL that fit value functions or compute policy
                   gradients, Decision Transformer simply outputs the optimal
                   actions by leveraging a causally masked Transformer. By
                   conditioning an autoregressive model on the desired return
                   (reward), past states, and actions, our Decision Transformer
                   model can generate future actions that achieve the desired
                   return. Despite its simplicity, Decision Transformer matches
                   or exceeds the performance of state-of-the-art model-free
                   offline RL baselines on Atari, OpenAI Gym, and Key-to-Door
                   tasks.",
  month         =  jun,
  year          =  2021,
  keywords      = "read;ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2106.01345"
}

@ARTICLE{Ashwood2022-ck,
  title     = "Mice alternate between discrete strategies during perceptual
               decision-making",
  author    = "Ashwood, Zoe C and Roy, Nicholas A and Stone, Iris R and Urai,
               Anne E and Churchland, Anne K and Pouget, Alexandre and Pillow,
               Jonathan W",
  abstract  = "Classical models of perceptual decision-making assume that
               subjects use a single, consistent strategy to form decisions, or
               that decision-making strategies evolve slowly over time. Here we
               present new analyses suggesting that this common view is
               incorrect. We analyzed data from mouse and human decision-making
               experiments and found that choice behavior relies on an
               interplay among multiple interleaved strategies. These
               strategies, characterized by states in a hidden Markov model,
               persist for tens to hundreds of trials before switching, and
               often switch multiple times within a session. The identified
               decision-making strategies were highly consistent across mice
               and comprised a single `engaged' state, in which decisions
               relied heavily on the sensory stimulus, and several biased
               states in which errors frequently occurred. These results
               provide a powerful alternate explanation for `lapses' often
               observed in rodent behavioral experiments, and suggest that
               standard measures of performance mask the presence of major
               changes in strategy across trials. The authors implement
               model-based analyses to uncover strategies used by mice and
               humans during sensory decision-making. Contrary to common
               wisdom, mice do not lapse and, instead, switch between sustained
               engaged and disengaged states.",
  journal   = "Nat. Neurosci.",
  publisher = "Nature Publishing Group",
  volume    =  25,
  number    =  2,
  pages     = "201--212",
  month     =  feb,
  year      =  2022,
  keywords  = "4 behavior;4 mouse;read next;unread;ERC Consolidator 2023",
  language  = "en"
}

@ARTICLE{Macpherson2021-re,
  title    = "Natural and Artificial Intelligence: A brief introduction to the
              interplay between {AI} and neuroscience research",
  author   = "Macpherson, Tom and Churchland, Anne and Sejnowski, Terry and
              DiCarlo, James and Kamitani, Yukiyasu and Takahashi, Hidehiko and
              Hikida, Takatoshi",
  abstract = "Neuroscience and artificial intelligence (AI) share a long
              history of collaboration. Advances in neuroscience, alongside
              huge leaps in computer processing power over the last few
              decades, have given rise to a new generation of in silico neural
              networks inspired by the architecture of the brain. These AI
              systems are now capable of many of the advanced perceptual and
              cognitive abilities of biological systems, including object
              recognition and decision making. Moreover, AI is now increasingly
              being employed as a tool for neuroscience research and is
              transforming our understanding of brain functions. In particular,
              deep learning has been used to model how convolutional layers and
              recurrent connections in the brain's cerebral cortex control
              important functions, including visual processing, memory, and
              motor control. Excitingly, the use of neuroscience-inspired AI
              also holds great promise for understanding how changes in brain
              networks result in psychopathologies, and could even be utilized
              in treatment regimes. Here we discuss recent advancements in four
              areas in which the relationship between neuroscience and AI has
              led to major advancements in the field; (1) AI models of working
              memory, (2) AI visual processing, (3) AI analysis of big
              neuroscience datasets, and (4) computational psychiatry.",
  journal  = "Neural Netw.",
  volume   =  144,
  pages    = "603--613",
  month    =  dec,
  year     =  2021,
  keywords = "Artificial intelligence; Neuroscience; Neural imaging; Visual
              processing; Working memory; Computational psychiatry;4 behavior;4
              mouse;skimmed;ERC Consolidator 2023"
}

@ARTICLE{Musall2019-kd,
  title     = "Single-trial neural dynamics are dominated by richly varied
               movements",
  author    = "Musall, Simon and Kaufman, Matthew T and Juavinett, Ashley L and
               Gluf, Steven and Churchland, Anne K",
  abstract  = "When experts are immersed in a task, do their brains prioritize
               task-related activity? Most efforts to understand neural
               activity during well-learned tasks focus on cognitive
               computations and task-related movements. We wondered whether
               task-performing animals explore a broader movement landscape and
               how this impacts neural activity. We characterized movements
               using video and other sensors and measured neural activity using
               widefield and two-photon imaging. Cortex-wide activity was
               dominated by movements, especially uninstructed movements not
               required for the task. Some uninstructed movements were aligned
               to trial events. Accounting for them revealed that neurons with
               similar trial-averaged activity often reflected utterly
               different combinations of cognitive and movement variables.
               Other movements occurred idiosyncratically, accounting for
               trial-by-trial fluctuations that are often considered `noise'.
               This held true throughout task-learning and for extracellular
               Neuropixels recordings that included subcortical areas. Our
               observations argue that animals execute expert decisions while
               performing richly varied, uninstructed movements that profoundly
               shape neural activity. The authors use a linear model to reveal
               how neural activity patterns are related to cognition or
               movements. They find that uninstructed movements dominate
               single-cell and population activity throughout the brain,
               outpacing task-related activity.",
  journal   = "Nat. Neurosci.",
  publisher = "Nature Publishing Group",
  volume    =  22,
  number    =  10,
  pages     = "1677--1686",
  month     =  sep,
  year      =  2019,
  keywords  = "4 behavior;4 mouse;skimmed;ERC Consolidator 2023",
  language  = "en"
}

@ARTICLE{Glaser2020-ax,
  title    = "Machine Learning for Neural Decoding",
  author   = "Glaser, Joshua I and Benjamin, Ari S and Chowdhury, Raeed H and
              Perich, Matthew G and Miller, Lee E and Kording, Konrad P",
  abstract = "Despite rapid advances in machine learning tools, the majority of
              neural decoding approaches still use traditional methods. Modern
              machine learning tools, which are versatile and easy to use, have
              the potential to significantly improve decoding performance. This
              tutorial describes how to effectively apply these algorithms for
              typical decoding problems. We provide descriptions, best
              practices, and code for applying common machine learning methods,
              including neural networks and gradient boosting. We also provide
              detailed comparisons of the performance of various methods at the
              task of decoding spiking activity in motor cortex, somatosensory
              cortex, and hippocampus. Modern methods, particularly neural
              networks and ensembles, significantly outperform traditional
              approaches, such as Wiener and Kalman filters. Improving the
              performance of neural decoding algorithms allows neuroscientists
              to better understand the information contained in a neural
              population and can help to advance engineering applications such
              as brain-machine interfaces. Our code package is available at
              github.com/kordinglab/neural\_decoding.",
  journal  = "eNeuro",
  volume   =  7,
  number   =  4,
  month    =  aug,
  year     =  2020,
  keywords = "Deep learning; Hippocampus; Machine learning; Motor cortex;
              Neural data analysis; Neural decoding; Somatosensory cortex;read
              next;unread;ERC Consolidator 2023",
  language = "en"
}

@INPROCEEDINGS{Batty2019-nc,
  title     = "{BehaveNet}: nonlinear embedding and Bayesian neural decoding of
               behavioral videos",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Batty, Eleanor and Whiteway, Matthew and Saxena, Shreya and
               Biderman, Dan and Abe, Taiga and Musall, Simon and Gillis,
               Winthrop and Markowitz, Jeffrey and Churchland, Anne and
               Cunningham, John P and Datta, Sandeep R and Linderman, Scott and
               Paninski, Liam",
  editor    = "Wallach, H and Larochelle, H and Beygelzimer, A and
               d\textbackslashtextquotesingle Alch{\'e}-Buc, F and Fox, E and
               Garnett, R",
  publisher = "Curran Associates, Inc.",
  volume    =  32,
  year      =  2019,
  keywords  = "4 behavior;read next;unread;ERC Consolidator 2023"
}

@ARTICLE{Bohnslav2021-gz,
  title    = "{DeepEthogram}, a machine learning pipeline for supervised
              behavior classification from raw pixels",
  author   = "Bohnslav, James P and Wimalasena, Nivanthika K and Clausing,
              Kelsey J and Dai, Yu Y and Yarmolinsky, David A and Cruz,
              Tom{\'a}s and Kashlan, Adam D and Chiappe, M Eugenia and Orefice,
              Lauren L and Woolf, Clifford J and Harvey, Christopher D",
  abstract = "Videos of animal behavior are used to quantify researcher-defined
              behaviors of interest to study neural function, gene mutations,
              and pharmacological therapies. Behaviors of interest are often
              scored manually, which is time-consuming, limited to few
              behaviors, and variable across researchers. We created
              DeepEthogram: software that uses supervised machine learning to
              convert raw video pixels into an ethogram, the behaviors of
              interest present in each video frame. DeepEthogram is designed to
              be general-purpose and applicable across species, behaviors, and
              video-recording hardware. It uses convolutional neural networks
              to compute motion, extract features from motion and images, and
              classify features into behaviors. Behaviors are classified with
              above 90\% accuracy on single frames in videos of mice and flies,
              matching expert-level human performance. DeepEthogram accurately
              predicts rare behaviors, requires little training data, and
              generalizes across subjects. A graphical interface allows
              beginning-to-end analysis without end-user programming.
              DeepEthogram's rapid, automatic, and reproducible labeling of
              researcher-defined behaviors of interest may accelerate and
              enhance supervised behavior analysis. Code is available at:
              https://github.com/jbohnslav/deepethogram.",
  journal  = "Elife",
  volume   =  10,
  month    =  sep,
  year     =  2021,
  keywords = "D. melanogaster; behavior analysis; computer vision; deep
              learning; mouse; neuroscience;read next;unread;ERC Consolidator
              2023",
  language = "en"
}

@ARTICLE{Katzner2019-di,
  title    = "{V1} microcircuits underlying mouse visual behavior",
  author   = "Katzner, Steffen and Born, Gregory and Busse, Laura",
  abstract = "Visual behavior is based on the concerted activity of neurons in
              visual areas, where sensory signals are integrated with top-down
              information. In the past decade, the advent of new tools, such as
              functional imaging of populations of identified single neurons,
              high-density electrophysiology, virus-assisted circuit mapping,
              and precisely timed, cell-type specific manipulations, has
              advanced our understanding of the neuronal microcircuits
              underlying visual behavior. Studies in head-fixed mice, where
              such tools can routinely be applied, begin to provide new
              insights into the neural code of primary visual cortex (V1)
              underlying visual perception, and the micro-circuits of
              attention, predictive processing, and learning.",
  journal  = "Curr. Opin. Neurobiol.",
  volume   =  58,
  pages    = "191--198",
  month    =  oct,
  year     =  2019,
  keywords = "read next;unread;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Huk2018-ez,
  title    = "Beyond {Trial-Based} Paradigms: Continuous Behavior, Ongoing
              Neural Activity, and Natural Stimuli",
  author   = "Huk, Alexander and Bonnen, Kathryn and He, Biyu J",
  abstract = "The vast majority of experiments examining perception and
              behavior are conducted using experimental paradigms that adhere
              to a rigid trial structure: each trial consists of a brief and
              discrete series of events and is regarded as independent from all
              other trials. The assumptions underlying this structure ignore
              the reality that natural behavior is rarely discrete, brain
              activity follows multiple time courses that do not necessarily
              conform to the trial structure, and the natural environment has
              statistical structure and dynamics that exhibit long-range
              temporal correlation. Modern advances in statistical modeling and
              analysis offer tools that make it feasible for experiments to
              move beyond rigid independent and identically distributed trial
              structures. Here we review literature that serves as evidence for
              the feasibility and advantages of moving beyond trial-based
              paradigms to understand the neural basis of perception and
              cognition. Furthermore, we propose a synthesis of these efforts,
              integrating the characterization of natural stimulus properties
              with measurements of continuous neural activity and behavioral
              outputs within the framework of sensory-cognitive-motor loops.
              Such a framework provides a basis for the study of natural
              statistics, naturalistic tasks, and/or slow fluctuations in brain
              activity, which should provide starting points for important
              generalizations of analytical tools in neuroscience and
              subsequent progress in understanding the neural basis of
              perception and cognition.",
  journal  = "J. Neurosci.",
  volume   =  38,
  number   =  35,
  pages    = "7551--7558",
  month    =  aug,
  year     =  2018,
  keywords = "read;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Datta2019-qj,
  title    = "Computational Neuroethology: A Call to Action",
  author   = "Datta, Sandeep Robert and Anderson, David J and Branson, Kristin
              and Perona, Pietro and Leifer, Andrew",
  abstract = "The brain is worthy of study because it is in charge of behavior.
              A flurry of recent technical advances in measuring and
              quantifying naturalistic behaviors provide an important
              opportunity for advancing brain science. However, the problem of
              understanding unrestrained behavior in the context of neural
              recordings and manipulations remains unsolved, and developing
              approaches to addressing this challenge is critical. Here we
              discuss considerations in computational neuroethology---the
              science of quantifying naturalistic behaviors for understanding
              the brain---and propose strategies to evaluate progress. We point
              to open questions that require resolution and call upon the
              broader systems neuroscience community to further develop and
              leverage measures of naturalistic, unrestrained behavior, which
              will enable us to more effectively probe the richness and
              complexity of the brain.",
  journal  = "Neuron",
  volume   =  104,
  number   =  1,
  pages    = "11--24",
  month    =  oct,
  year     =  2019,
  keywords = "read next;unread;ERC Consolidator 2023"
}

@ARTICLE{Chiappa2022-tj,
  title         = "{DMAP}: a Distributed Morphological Attention Policy for
                   Learning to Locomote with a Changing Body",
  author        = "Chiappa, Alberto Silvio and Vargas, Alessandro Marin and
                   Mathis, Alexander",
  abstract      = "Biological and artificial agents need to deal with constant
                   changes in the real world. We study this problem in four
                   classical continuous control environments, augmented with
                   morphological perturbations. Learning to locomote when the
                   length and the thickness of different body parts vary is
                   challenging, as the control policy is required to adapt to
                   the morphology to successfully balance and advance the
                   agent. We show that a control policy based on the
                   proprioceptive state performs poorly with highly variable
                   body configurations, while an (oracle) agent with access to
                   a learned encoding of the perturbation performs
                   significantly better. We introduce DMAP, a
                   biologically-inspired, attention-based policy network
                   architecture. DMAP combines independent proprioceptive
                   processing, a distributed policy with individual controllers
                   for each joint, and an attention mechanism, to dynamically
                   gate sensory information from different body parts to
                   different controllers. Despite not having access to the
                   (hidden) morphology information, DMAP can be trained
                   end-to-end in all the considered environments, overall
                   matching or surpassing the performance of an oracle agent.
                   Thus DMAP, implementing principles from biological motor
                   control, provides a strong inductive bias for learning
                   challenging sensorimotor tasks. Overall, our work
                   corroborates the power of these principles in challenging
                   locomotion tasks.",
  month         =  sep,
  year          =  2022,
  keywords      = "unread;ERC Consolidator 2023",
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2209.14218"
}

@ARTICLE{Holmgren2021-jv,
  title    = "Visual pursuit behavior in mice maintains the pursued prey on the
              retinal region with least optic flow",
  author   = "Holmgren, Carl D and Stahr, Paul and Wallace, Damian J and Voit,
              Kay-Michael and Matheson, Emily J and Sawinski, Juergen and
              Bassetto, Giacomo and Kerr, Jason Nd",
  abstract = "Mice have a large visual field that is constantly stabilized by
              vestibular ocular reflex (VOR) driven eye rotations that counter
              head-rotations. While maintaining their extensive visual coverage
              is advantageous for predator detection, mice also track and
              capture prey using vision. However, in the freely moving animal
              quantifying object location in the field of view is challenging.
              Here, we developed a method to digitally reconstruct and quantify
              the visual scene of freely moving mice performing a visually
              based prey capture task. By isolating the visual sense and
              combining a mouse eye optic model with the head and eye
              rotations, the detailed reconstruction of the digital environment
              and retinal features were projected onto the corneal surface for
              comparison, and updated throughout the behavior. By quantifying
              the spatial location of objects in the visual scene and their
              motion throughout the behavior, we show that the prey image
              consistently falls within a small area of the VOR-stabilized
              visual field. This functional focus coincides with the region of
              minimal optic flow within the visual field and consequently area
              of minimal motion-induced image-blur, as during pursuit mice ran
              directly toward the prey. The functional focus lies in the
              upper-temporal part of the retina and coincides with the reported
              high density-region of Alpha-ON sustained retinal ganglion cells.",
  journal  = "Elife",
  volume   =  10,
  month    =  oct,
  year     =  2021,
  keywords = "eye movements; freely moving behavior; methods development;
              mouse; neuroscience; optic flow; prey capture; vision;4
              behavior;4 mouse;read next;unread;ERC Consolidator 2023",
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Horrocks2023-ik,
  title    = "Walking humans and running mice: perception and neural encoding
              of optic flow during self-motion",
  author   = "Horrocks, Edward A B and Mareschal, Isabelle and Saleem, Aman B",
  abstract = "Locomotion produces full-field optic flow that often dominates
              the visual motion inputs to an observer. The perception of optic
              flow is in turn important for animals to guide their heading and
              interact with moving objects. Understanding how locomotion
              influences optic flow processing and perception is therefore
              essential to understand how animals successfully interact with
              their environment. Here, we review research investigating how
              perception and neural encoding of optic flow are altered during
              self-motion, focusing on locomotion. Self-motion has been found
              to influence estimation and sensitivity for optic flow speed and
              direction. Nonvisual self-motion signals also increase
              compensation for self-driven optic flow when parsing the visual
              motion of moving objects. The integration of visual and nonvisual
              self-motion signals largely follows principles of Bayesian
              inference and can improve the precision and accuracy of
              self-motion perception. The calibration of visual and nonvisual
              self-motion signals is dynamic, reflecting the changing
              visuomotor contingencies across different environmental contexts.
              Throughout this review, we consider experimental research using
              humans, non-human primates and mice. We highlight experimental
              challenges and opportunities afforded by each of these species
              and draw parallels between experimental findings. These findings
              reveal a profound influence of locomotion on optic flow
              processing and perception across species. This article is part of
              a discussion meeting issue 'New approaches to 3D vision'.",
  journal  = "Philos. Trans. R. Soc. Lond. B Biol. Sci.",
  volume   =  378,
  number   =  1869,
  pages    = "20210450",
  month    =  jan,
  year     =  2023,
  keywords = "human vision‌; locomotion; mouse vision; optic flow;
              psychophysics;4 behavior;read;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Dyballa2018-zr,
  title    = "Flow stimuli reveal ecologically appropriate responses in mouse
              visual cortex",
  author   = "Dyballa, Luciano and Hoseini, Mahmood S and Dadarlat, Maria C and
              Zucker, Steven W and Stryker, Michael P",
  abstract = "Assessments of the mouse visual system based on spatial-frequency
              analysis imply that its visual capacity is low, with few neurons
              responding to spatial frequencies greater than 0.5 cycles per
              degree. However, visually mediated behaviors, such as prey
              capture, suggest that the mouse visual system is more precise. We
              introduce a stimulus class---visual flow patterns---that is more
              like what the mouse would encounter in the natural world than are
              sine-wave gratings but is more tractable for analysis than are
              natural images. We used 128-site silicon microelectrodes to
              measure the simultaneous responses of single neurons in the
              primary visual cortex (V1) of alert mice. While holding
              temporal-frequency content fixed, we explored a class of drifting
              patterns of black or white dots that have energy only at higher
              spatial frequencies. These flow stimuli evoke strong visually
              mediated responses well beyond those predicted by
              spatial-frequency analysis. Flow responses predominate in higher
              spatial-frequency ranges (0.15--1.6 cycles per degree), many are
              orientation or direction selective, and flow responses of many
              neurons depend strongly on sign of contrast. Many cells exhibit
              distributed responses across our stimulus ensemble. Together,
              these results challenge conventional linear approaches to visual
              processing and expand our understanding of the mouse's visual
              capacity to behaviorally relevant ranges.",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  115,
  number   =  44,
  pages    = "11304--11309",
  year     =  2018,
  keywords = "4 behavior;read next;unread;ERC Consolidator 2023"
}

@ARTICLE{Ayaz2013-vv,
  title    = "Locomotion controls spatial integration in mouse visual cortex",
  author   = "Ayaz, Asl{\i} and Saleem, Aman B and Sch{\"o}lvinck, Marieke L
              and Carandini, Matteo",
  abstract = "Growing evidence indicates that responses in sensory cortex are
              modulated by factors beyond direct sensory stimulation. In
              primary visual cortex (V1), for instance, responses increase with
              locomotion. Here we show that this increase is accompanied by a
              profound change in spatial integration. We recorded from V1
              neurons in head-fixed mice placed on a spherical treadmill. We
              characterized spatial integration and found that the responses of
              most neurons were suppressed by large stimuli. As in primates,
              this surround suppression increased with stimulus contrast. These
              effects were captured by a divisive normalization model, where
              the numerator originates from a central region driving the neuron
              and the denominator originates from a larger suppressive field.
              We then studied the effects of locomotion and found that it
              markedly reduced surround suppression, allowing V1 neurons to
              integrate over larger regions of visual space. Locomotion had two
              main effects: it increased spontaneous activity, and it weakened
              the suppressive signals mediating normalization, relative to the
              driving signals. We conclude that a fundamental aspect of visual
              processing, spatial integration, is controlled by an apparently
              unrelated factor, locomotion. This control might operate through
              the mechanisms that are in place to deliver surround suppression.",
  journal  = "Curr. Biol.",
  volume   =  23,
  number   =  10,
  pages    = "890--894",
  month    =  may,
  year     =  2013,
  keywords = "4 mouse;read next;unread;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Andermann2011-vw,
  title    = "Functional specialization of mouse higher visual cortical areas",
  author   = "Andermann, Mark L and Kerlin, Aaron M and Roumis, Demetris K and
              Glickfeld, Lindsey L and Reid, R Clay",
  abstract = "The mouse is emerging as an important model for understanding how
              sensory neocortex extracts cues to guide behavior, yet little is
              known about how these cues are processed beyond primary cortical
              areas. Here, we used two-photon calcium imaging in awake mice to
              compare visual responses in primary visual cortex (V1) and in two
              downstream target areas, AL and PM. Neighboring V1 neurons had
              diverse stimulus preferences spanning five octaves in spatial and
              temporal frequency. By contrast, AL and PM neurons responded best
              to distinct ranges of stimulus parameters. Most strikingly, AL
              neurons preferred fast-moving stimuli while PM neurons preferred
              slow-moving stimuli. By contrast, neurons in V1, AL, and PM
              demonstrated similar selectivity for stimulus orientation but not
              for stimulus direction. Based on these findings, we predict that
              area AL helps guide behaviors involving fast-moving stimuli
              (e.g., optic flow), while area PM helps guide behaviors involving
              slow-moving objects.",
  journal  = "Neuron",
  volume   =  72,
  number   =  6,
  pages    = "1025--1039",
  month    =  dec,
  year     =  2011,
  keywords = "4 behavior;4 mouse;read next;unread;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Vinck2015-wy,
  title     = "Arousal and locomotion make distinct contributions to cortical
               activity patterns and visual encoding",
  author    = "Vinck, Martin and Batista-Brito, Renata and Knoblich, Ulf and
               Cardin, Jessica A",
  abstract  = "Spontaneous and sensory-evoked cortical activity is highly
               state-dependent, yet relatively little is known about
               transitions between distinct waking states. Patterns of activity
               in mouse V1 differ dramatically between quiescence and
               locomotion, but this difference could be explained by either
               motor feedback or a change in arousal levels. We recorded single
               cells and local field potentials from area V1 in mice head-fixed
               on a running wheel and monitored pupil diameter to assay
               arousal. Using naturally occurring and induced state
               transitions, we dissociated arousal and locomotion effects in
               V1. Arousal suppressed spontaneous firing and strongly altered
               the temporal patterning of population activity. Moreover,
               heightened arousal increased the signal-to-noise ratio of visual
               responses and reduced noise correlations. In contrast, increased
               firing in anticipation of and during movement was attributable
               to locomotion effects. Our findings suggest complementary roles
               of arousal and locomotion in promoting functional flexibility in
               cortical circuits.",
  journal   = "Neuron",
  publisher = "Elsevier",
  volume    =  86,
  number    =  3,
  pages     = "740--754",
  month     =  may,
  year      =  2015,
  keywords  = "4 behavior;read next;unread;ERC Consolidator 2023",
  language  = "en"
}

@ARTICLE{Franklin2011-lr,
  title    = "Computational mechanisms of sensorimotor control",
  author   = "Franklin, David W and Wolpert, Daniel M",
  abstract = "In order to generate skilled and efficient actions, the motor
              system must find solutions to several problems inherent in
              sensorimotor control, including nonlinearity, nonstationarity,
              delays, redundancy, uncertainty, and noise. We review these
              problems and five computational mechanisms that the brain may use
              to limit their deleterious effects: optimal feedback control,
              impedance control, predictive control, Bayesian decision theory,
              and sensorimotor learning. Together, these computational
              mechanisms allow skilled and fluent sensorimotor behavior.",
  journal  = "Neuron",
  volume   =  72,
  number   =  3,
  pages    = "425--442",
  month    =  nov,
  year     =  2011,
  keywords = "read next;unread;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Todorov2004-yb,
  title     = "Optimality principles in sensorimotor control",
  author    = "Todorov, Emanuel",
  abstract  = "The sensorimotor system is a product of evolution, development,
               learning and adaptation---which work on different time scales to
               improve behavioral performance. Consequently, many theories of
               motor function are based on 'optimal performance': they quantify
               task goals as cost functions, and apply the sophisticated tools
               of optimal control theory to obtain detailed behavioral
               predictions. The resulting models, although not without
               limitations, have explained more empirical phenomena than any
               other class. Traditional emphasis has been on optimizing desired
               movement trajectories while ignoring sensory feedback. Recent
               work has redefined optimality in terms of feedback control laws,
               and focused on the mechanisms that generate behavior online.
               This approach has allowed researchers to fit previously
               unrelated concepts and observations into what may become a
               unified theoretical framework for interpreting motor function.
               At the heart of the framework is the relationship between
               high-level goals, and the real-time sensorimotor control
               strategies most suitable for accomplishing those goals.",
  journal   = "Nat. Neurosci.",
  publisher = "Nature Publishing Group",
  volume    =  7,
  number    =  9,
  pages     = "907--915",
  month     =  aug,
  year      =  2004,
  keywords  = "skimmed;4 behavior;ERC Consolidator 2023",
  language  = "en"
}

@ARTICLE{Mathis2021-zo,
  title    = "Motor control: Neural correlates of optimal feedback control
              theory",
  author   = "Mathis, Mackenzie W and Schneider, Steffen",
  abstract = "Recent work is revealing neural correlates of a leading theory of
              motor control. By linking an elegant series of behavioral
              experiments with neural inactivation in macaques with
              computational models, a new study shows that premotor and
              parietal areas can be mapped onto a model for optimal feedback
              control.",
  journal  = "Curr. Biol.",
  volume   =  31,
  number   =  7,
  pages    = "R356--R358",
  month    =  apr,
  year     =  2021,
  keywords = "4 behavior;skimmed;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Conant1970-td,
  title     = "Every good regulator of a system must be a model of that system",
  author    = "Conant, Roger C and Ross Ashby, W",
  abstract  = "The design of a complex regulator often includes the making of a
               model of the system to be regulated. The making of such a model
               has hitherto been regarded as optional, as merely one of many
               possible ways. In this paper a theorem is presented which shows,
               under very broad conditions, that any regulator that is
               maximally both successful and simple must be isomorphic with the
               system being regulated. (The exact assumptions are given.)
               Making a model is thus necessary. The theorem has the
               interesting corollary that the living brain, so far as it is to
               be successful and efficient as a regulator for survival, must
               proceed, in learning, by the formation of a model (or models) of
               its environment.",
  journal   = "Int. J. Syst. Sci.",
  publisher = "Taylor \& Francis",
  volume    =  1,
  number    =  2,
  pages     = "89--97",
  month     =  oct,
  year      =  1970,
  keywords  = "unread;ERC Consolidator 2023"
}

@UNPUBLISHED{Schnabel2018-tb,
  title    = "Feedforward and feedback processing during figure-ground
              perception in mice",
  author   = "Schnabel, Ulf H and Kirchberger, Lisa and van Beest, Enny H and
              Mukherjee, Sreedeep and Barsegyan, Areg and Lorteije, Jeannette A
              M and van der Togt, Chris and Self, Matthew W and Roelfsema,
              Pieter R",
  abstract = "Abstract The segregation of figures from the background is an
              important first step in the analysis of a visual scene.
              Figure-ground segregation is thought to rely on interactions
              between the primary visual cortex (area V1) and higher visual
              areas. Upon presentation of a new image, the initial V1 responses
              reflect the information in the neurons9 receptive fields (RFs),
              whereas later activity is context-dependent so that the
              representations of figures are enhanced relative to the
              background1-3. It is unknown if the figural response enhancement
              in V1 plays a role in perception and the mechanisms that produce
              it are not well understood. We trained mice to report a
              contrast-defined figure or a figure on a textured background that
              required figure-ground segregation, and optogenetically silenced
              V1 activity at different time-points. Suppression of early V1
              activity interfered with both visual tasks, but suppression of
              the later activity selectively interfered with figure-ground
              perception. Using widefield imaging, we observed that figures
              also elicited stronger activity than the background in higher
              visual areas, and we used optogenetics to demonstrate that the
              extra activity is fed back to enhance the V1 figure
              representation. Within V1, figures increased the activity of
              pyramidal cells, parvalbumin (PV) and vasoactive intestinal
              peptide (VIP) positive interneurons but they decreased the
              activity of somatostatin-positive (SOM) interneurons. These
              results support the view that VIP interneurons inhibit
              SOM-interneurons to disinhibit the cortical column4. Our results
              demonstrate that figure-ground modulation of V1 neurons
              contributes to perception. We expect that the observed
              interactions between lower and higher brain regions generalize to
              other sensory modalities and that they will inform
              neurobiologically realistic models of scene perception.One
              sentence summary Figure-ground perception in mice depends on
              reciprocal interactions between the primary visual cortex and
              higher cortical areas, with specific roles for PV-, VIP- and
              SOM-positive interneurons.",
  journal  = "bioRxiv",
  pages    = "456459",
  month    =  oct,
  year     =  2018,
  keywords = "4 mouse;read next;unread;ERC Consolidator 2023",
  language = "en"
}

@ARTICLE{Murgas2020-as,
  title    = "Unique Spatial Integration in Mouse Primary Visual Cortex and
              Higher Visual Areas",
  author   = "Murgas, Kevin A and Wilson, Ashley M and Michael, Valerie and
              Glickfeld, Lindsey L",
  abstract = "Neurons in the visual system integrate over a wide range of
              spatial scales. This diversity is thought to enable both local
              and global computations. To understand how spatial information is
              encoded across the mouse visual system, we use two-photon imaging
              to measure receptive fields (RFs) and size-tuning in primary
              visual cortex (V1) and three downstream higher visual areas
              (HVAs: LM (lateromedial), AL (anterolateral), and PM
              (posteromedial)) in mice of both sexes. Neurons in PM, compared
              with V1 or the other HVAs, have significantly larger RF sizes and
              less surround suppression, independent of stimulus eccentricity
              or contrast. To understand how this specialization of RFs arises
              in the HVAs, we measured the spatial properties of V1 inputs to
              each area. Spatial integration of V1 axons was remarkably similar
              across areas and significantly different from the tuning of
              neurons in their target HVAs. Thus, unlike other visual features
              studied in this system, specialization of spatial integration in
              PM cannot be explained by specific projections from V1 to the
              HVAs. Further, the differences in RF properties could not be
              explained by differences in convergence of V1 inputs to the HVAs.
              Instead, our data suggest that distinct inputs from other areas
              or connectivity within PM may support the area's unique ability
              to encode global features of the visual scene, whereas V1, LM,
              and AL may be more specialized for processing local
              features.SIGNIFICANCE STATEMENT Surround suppression is a common
              feature of visual processing whereby large stimuli are less
              effective at driving neuronal responses than smaller stimuli.
              This is thought to enhance efficiency in the population code and
              enable higher-order processing of visual information, such as
              figure-ground segregation. However, this comes at the expense of
              global computations. Here we find that surround suppression is
              not equally represented across mouse visual areas: primary visual
              cortex has substantially more surround suppression than higher
              visual areas, and one higher area has significantly less
              suppression than two others examined, suggesting that these areas
              have distinct functional roles. Thus, we have identified a novel
              dimension of specialization in the mouse visual cortex that may
              enable both local and global computations.",
  journal  = "J. Neurosci.",
  volume   =  40,
  number   =  9,
  pages    = "1862--1873",
  month    =  feb,
  year     =  2020,
  keywords = "calcium imaging; contrast; mouse visual cortex; normalization;
              size tuning; surround suppression;4 behavior;4 mouse;read
              next;unread;ERC Consolidator 2023",
  language = "en"
}

@UNPUBLISHED{Cobos2022-rr,
  title    = "It takes neurons to understand neurons: Digital twins of visual
              cortex synthesize neural metamers",
  author   = "Cobos, Erick and Muhammad, Taliah and Fahey, Paul G and Ding,
              Zhiwei and Ding, Zhuokun and Reimer, Jacob and Sinz, Fabian H and
              Tolias, Andreas S",
  abstract = "Metamers, images that are perceived as equal, are a useful tool
              to study representations of natural images in biological and
              artificial vision systems. We synthesized metamers for the mouse
              visual system by inverting a deep encoding model to find an image
              that matched the observed neural activity to the original
              presented image. When testing the resulting images in
              physiological experiments we found that they most closely
              reproduced the neural activity of the original image when
              compared to other decoding methods, even when tested in a
              different animal whose neural activity was not used to produce
              the metamer. This demonstrates that deep encoding models do
              capture general characteristic properties of biological visual
              systems and can be used to define a meaningful perceptual loss
              for the visual system. \#\#\# Competing Interest Statement The
              authors have declared no competing interest.",
  journal  = "bioRxiv",
  pages    = "2022.12.09.519708",
  month    =  dec,
  year     =  2022,
  keywords = "own;ERC Consolidator 2023",
  language = "en"
}
